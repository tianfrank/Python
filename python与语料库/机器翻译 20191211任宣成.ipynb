{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 玩具数据\n",
    "步骤\n",
    "\n",
    "数据处理\n",
    "\n",
    "模型定义\n",
    "\n",
    "训练循环\n",
    "\n",
    "交互测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#问题1： cmn文件里面的标注代表什么\n",
    "#问题2：he is lazy的图片代表什么\n",
    "#问题3：BPE下面的iteration10代表什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入需要的工具包\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "# 我们使用pytorch框架完成这个任务\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 计算设备对象，后续的操作中会使用该对象。如果CUDA可用的话，将使用GPU进行计算，否则使用CPU。\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def reset_seed():\n",
    "    random.seed(1211)\n",
    "    torch.manual_seed(1211)\n",
    "    torch.cuda.manual_seed(1211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text...\n",
      "Read 1030 pairs\n",
      "en: 884 token types\n",
      "zh: 870 token types\n",
      "['he s very angry with you .', '他 对 你 非 常 生 气 。']\n"
     ]
    }
   ],
   "source": [
    "SOS=0\n",
    "EOS=1\n",
    "MAX_LENGTH = 10\n",
    "PREFIXS = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def clean_latin_text(s):\n",
    "    # 按空格分词\n",
    "    s = s.lower().strip() # 小写并清除首尾空白字符\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # 标点前添加空格\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) # 清除其它符号\n",
    "    return s\n",
    "\n",
    "def clean_zh_text(s):\n",
    "    # 分字\n",
    "    s = s.strip()\n",
    "    s = ' '.join([char for char in s])\n",
    "    return s\n",
    "\n",
    "\n",
    "def filter_by_en(en, zh):\n",
    "    # 只保留简单的句子\n",
    "    return (len(en.split(' ')) < MAX_LENGTH and\n",
    "        len(zh.split(' ')) < MAX_LENGTH and\n",
    "        en.startswith(PREFIXS))\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {'<SOS>': 0, '<EOS>': 1}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: '<SOS>', 1: '<EOS>'}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2index)\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = len(self)\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[len(self)-1] = word\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "#def read_text(filename='05/cmn.txt'): 任宣成原来的\n",
    "def read_text(filename='D:/NLP/机器学习/cmn-eng/cmn.txt',encoding='utf-8'):\n",
    "    print('reading text...')\n",
    "    ens = []\n",
    "    zhs = []\n",
    "\n",
    "    vocab_en = Vocab('en')\n",
    "    vocab_zh = Vocab('zh')\n",
    "\n",
    "    # 用于繁简转换\n",
    "    import zhconv\n",
    "\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split('\\t')\n",
    "            en, zh = parts[0], parts[1]\n",
    "            en = clean_latin_text(en)\n",
    "            zh = zhconv.convert(zh, 'zh-cn')\n",
    "            zh = clean_zh_text(zh)\n",
    "            if filter_by_en(en, zh):\n",
    "                ens.append(en)\n",
    "                zhs.append(zh)\n",
    "                vocab_en.add_sentence(en)\n",
    "                vocab_zh.add_sentence(zh)\n",
    "\n",
    "    print(f'Read {len(ens)} pairs')\n",
    "    print(f'en: {len(vocab_en)} token types')\n",
    "    print(f'zh: {len(vocab_zh)} token types')\n",
    "    return ens, zhs, vocab_en, vocab_zh\n",
    "\n",
    "\n",
    "ens, zhs, vocab_en, vocab_zh = read_text()\n",
    "pairs = [[en, zh] for en, zh in zip(ens, zhs)]\n",
    "\n",
    "reset_seed()\n",
    "ex = random.choice(pairs)\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zhconv\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/ff/befc05572aa53eaf542c75a7d7b18d66c24c418ba0cc9202d383e58f6c70/zhconv-1.4.0-py2.py3-none-any.whl (179kB)\n",
      "Installing collected packages: zhconv\n",
      "Successfully installed zhconv-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install zhconv，提示安装繁简转换的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__() # 必须在添加参数前初始化基类\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 输入单位为句子\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, 1)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # 输入单位为词\n",
    "        # hidden should be [1, 1, hidden]\n",
    "        # encoder_outputs shoud be [1, src_len, hidden]\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        # [1, 1, hidden], [1, 1, hidden]\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        # [1, src_len]\n",
    "        # 这里Attention的实现方法是，将解码器的隐状态与编码器的输出合并\n",
    "        # 然后经过一个变化，输出每个位置对的分数\n",
    "        attn_weights = self.attn(torch.cat([hidden.expand_as(encoder_outputs), encoder_outputs], dim=-1)).view(1, -1)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "\n",
    "        # [1, hidden] 这里是对编码器输出进行加权平均\n",
    "        attn_applied = torch.mm(attn_weights, encoder_outputs[0])\n",
    "\n",
    "        # [1, hidden]\n",
    "        output = F.relu(self.attn_combine(torch.cat([output[0], attn_applied], dim=1)))\n",
    "\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 14,  15,  86, 128, 319,  51,   5,   1]], device='cuda:0'), tensor([[  0,  19,  28,  76, 187, 188,   6, 194,   5,   1]], device='cuda:0'))\n",
      "('he s very angry with you . <EOS>', '<SOS> 他 对 你 非 常 生 气 。 <EOS>')\n"
     ]
    }
   ],
   "source": [
    "# 数字化句子对\n",
    "def binarize_text(line, vocab):\n",
    "    idxs =  [vocab.word2index[word] for word in line.split(' ')]\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def string_text(tensor, vocab):\n",
    "    idxs = tensor.tolist()[0]\n",
    "    return ' '.join([vocab.index2word[idx] for idx in idxs])\n",
    "\n",
    "def tensor_from_pair(pair):\n",
    "    return binarize_text(pair[0] + ' <EOS>', vocab_en), binarize_text('<SOS> ' + pair[1] + ' <EOS>', vocab_zh)\n",
    "\n",
    "def recover_from_tensor(pair):\n",
    "    return string_text(pair[0], vocab_en), string_text(pair[1], vocab_zh)\n",
    "\n",
    "# 注意，为了使得模型更好的捕获控制信号，我们会额外添加<SOS>和<EOS>符号\n",
    "print(tensor_from_pair(ex))\n",
    "print(recover_from_tensor(tensor_from_pair(ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个训练步骤，由前向传播、反向传播和参数更新构成\n",
    "def train_step(input_tensor, target_tensor,\n",
    "          encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer,\n",
    "          criterion):\n",
    "\n",
    "    # optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    # encode [1, src_len, hidden_size], [1, 1, hidden_size]\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    # decode\n",
    "    decoder_outputs = []\n",
    "    decoder_hidden = encoder_hidden # [1, 1, hidden_size]\n",
    "\n",
    "    # Teacher forcing\n",
    "    for di in range(target_length - 1):\n",
    "        # [1, out], [1, 1, hidden], [1, src_len]\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            target_tensor[:, di], decoder_hidden, encoder_outputs)\n",
    "        decoder_outputs.append(decoder_output)\n",
    "\n",
    "    decoder_outputs = torch.stack(decoder_outputs, dim=1)[0] # [tgt_len, vocab]\n",
    "    decoder_targets = target_tensor[0, 1:] # [tgt_len]\n",
    "    loss = criterion(decoder_outputs, decoder_targets)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / (target_length - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整个训练的循环，我们训练固定的次数\n",
    "def train(encoder, decoder, n_iters, print_every=100,\n",
    "               learning_rate=0.001):\n",
    "    reset_seed()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    # 初始化优化器和优化目标\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = tensor_from_pair(random.choice(pairs)) # 随机选择一个句子对进行训练\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train_step(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f'| iter {iter:03d}/{n_iters:03d} | loss {print_loss_avg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| iter 100/10000 | loss 0.6155\n",
      "| iter 200/10000 | loss 0.4920\n",
      "| iter 300/10000 | loss 0.4731\n",
      "| iter 400/10000 | loss 0.4806\n",
      "| iter 500/10000 | loss 0.4508\n",
      "| iter 600/10000 | loss 0.4479\n",
      "| iter 700/10000 | loss 0.4428\n",
      "| iter 800/10000 | loss 0.4267\n",
      "| iter 900/10000 | loss 0.4364\n",
      "| iter 1000/10000 | loss 0.3911\n",
      "| iter 1100/10000 | loss 0.3953\n",
      "| iter 1200/10000 | loss 0.3704\n",
      "| iter 1300/10000 | loss 0.3774\n",
      "| iter 1400/10000 | loss 0.3633\n",
      "| iter 1500/10000 | loss 0.3672\n",
      "| iter 1600/10000 | loss 0.3461\n",
      "| iter 1700/10000 | loss 0.3808\n",
      "| iter 1800/10000 | loss 0.3316\n",
      "| iter 1900/10000 | loss 0.3598\n",
      "| iter 2000/10000 | loss 0.3130\n",
      "| iter 2100/10000 | loss 0.2990\n",
      "| iter 2200/10000 | loss 0.3019\n",
      "| iter 2300/10000 | loss 0.2984\n",
      "| iter 2400/10000 | loss 0.2756\n",
      "| iter 2500/10000 | loss 0.2607\n",
      "| iter 2600/10000 | loss 0.2714\n",
      "| iter 2700/10000 | loss 0.2668\n",
      "| iter 2800/10000 | loss 0.2765\n",
      "| iter 2900/10000 | loss 0.2305\n",
      "| iter 3000/10000 | loss 0.2374\n",
      "| iter 3100/10000 | loss 0.2258\n",
      "| iter 3200/10000 | loss 0.2244\n",
      "| iter 3300/10000 | loss 0.2126\n",
      "| iter 3400/10000 | loss 0.2029\n",
      "| iter 3500/10000 | loss 0.2073\n",
      "| iter 3600/10000 | loss 0.2014\n",
      "| iter 3700/10000 | loss 0.2023\n",
      "| iter 3800/10000 | loss 0.1843\n",
      "| iter 3900/10000 | loss 0.1748\n",
      "| iter 4000/10000 | loss 0.1550\n",
      "| iter 4100/10000 | loss 0.1705\n",
      "| iter 4200/10000 | loss 0.1522\n",
      "| iter 4300/10000 | loss 0.1824\n",
      "| iter 4400/10000 | loss 0.1618\n",
      "| iter 4500/10000 | loss 0.1440\n",
      "| iter 4600/10000 | loss 0.1345\n",
      "| iter 4700/10000 | loss 0.1524\n",
      "| iter 4800/10000 | loss 0.1229\n",
      "| iter 4900/10000 | loss 0.1404\n",
      "| iter 5000/10000 | loss 0.1490\n",
      "| iter 5100/10000 | loss 0.1310\n",
      "| iter 5200/10000 | loss 0.1415\n",
      "| iter 5300/10000 | loss 0.1182\n",
      "| iter 5400/10000 | loss 0.1176\n",
      "| iter 5500/10000 | loss 0.0997\n",
      "| iter 5600/10000 | loss 0.1040\n",
      "| iter 5700/10000 | loss 0.0821\n",
      "| iter 5800/10000 | loss 0.0916\n",
      "| iter 5900/10000 | loss 0.1045\n",
      "| iter 6000/10000 | loss 0.1003\n",
      "| iter 6100/10000 | loss 0.0792\n",
      "| iter 6200/10000 | loss 0.0759\n",
      "| iter 6300/10000 | loss 0.0846\n",
      "| iter 6400/10000 | loss 0.0797\n",
      "| iter 6500/10000 | loss 0.0719\n",
      "| iter 6600/10000 | loss 0.1024\n",
      "| iter 6700/10000 | loss 0.0882\n",
      "| iter 6800/10000 | loss 0.0627\n",
      "| iter 6900/10000 | loss 0.0756\n",
      "| iter 7000/10000 | loss 0.0628\n",
      "| iter 7100/10000 | loss 0.0616\n",
      "| iter 7200/10000 | loss 0.0610\n",
      "| iter 7300/10000 | loss 0.0537\n",
      "| iter 7400/10000 | loss 0.0721\n",
      "| iter 7500/10000 | loss 0.0523\n",
      "| iter 7600/10000 | loss 0.0501\n",
      "| iter 7700/10000 | loss 0.0374\n",
      "| iter 7800/10000 | loss 0.0474\n",
      "| iter 7900/10000 | loss 0.0521\n",
      "| iter 8000/10000 | loss 0.0421\n",
      "| iter 8100/10000 | loss 0.0415\n",
      "| iter 8200/10000 | loss 0.0336\n",
      "| iter 8300/10000 | loss 0.0421\n",
      "| iter 8400/10000 | loss 0.0392\n",
      "| iter 8500/10000 | loss 0.0380\n",
      "| iter 8600/10000 | loss 0.0432\n",
      "| iter 8700/10000 | loss 0.0380\n",
      "| iter 8800/10000 | loss 0.0534\n",
      "| iter 8900/10000 | loss 0.0319\n",
      "| iter 9000/10000 | loss 0.0276\n",
      "| iter 9100/10000 | loss 0.0335\n",
      "| iter 9200/10000 | loss 0.0358\n",
      "| iter 9300/10000 | loss 0.0295\n",
      "| iter 9400/10000 | loss 0.0396\n",
      "| iter 9500/10000 | loss 0.0210\n",
      "| iter 9600/10000 | loss 0.0255\n",
      "| iter 9700/10000 | loss 0.0299\n",
      "| iter 9800/10000 | loss 0.0292\n",
      "| iter 9900/10000 | loss 0.0246\n",
      "| iter 10000/10000 | loss 0.0406\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "hidden_size = 128\n",
    "\n",
    "encoder1 = EncoderRNN(len(vocab_en), hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(vocab_zh)).to(device)\n",
    "\n",
    "train(encoder1, attn_decoder1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, sentence):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = binarize_text(clean_latin_text(sentence) + ' <EOS>', vocab_en)\n",
    "        input_length = input_tensor.size()[1]\n",
    "\n",
    "        # encode [1, src_len, hidden_size], [1, 1, hidden_size]\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        # decode\n",
    "        decoder_words = ['<SOS>']\n",
    "        decoder_hidden = encoder_hidden # [1, 1, hidden_size]\n",
    "        decoder_attentions = []\n",
    "\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            last_word = decoder_words[-1]\n",
    "            last_word_idx = vocab_zh.word2index[last_word]\n",
    "            decoder_input = torch.tensor([[last_word_idx]], device=device)\n",
    "\n",
    "            # [1, out], [1, 1, hidden], [1, src_len]\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions.append(decoder_attention)\n",
    "\n",
    "            word_idx = torch.argmax(decoder_output[0])\n",
    "            word = vocab_zh.index2word[word_idx.item()]\n",
    "            decoder_words.append(word)\n",
    "            if word == '<EOS>':\n",
    "                break\n",
    "\n",
    "        # [tgt_len, src_len]\n",
    "        decoder_attentions = torch.cat(decoder_attentions, dim=0).tolist()\n",
    "\n",
    "        return decoder_words, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> he s very angry with you .\n",
      "= 他 对 你 非 常 生 气 。\n",
      "< 他 非 常 生 气 。 <EOS>\n",
      "\n",
      "> you are responsible for the result .\n",
      "= 你 要 对 结 果 负 责 。\n",
      "< 你 要 对 结 果 负 责 。 <EOS>\n",
      "\n",
      "> he is to come to my house tonight .\n",
      "= 他 今 晚 会 来 我 家 。\n",
      "< 他 今 晚 会 来 我 家 。 <EOS>\n",
      "\n",
      "> they re digging a hole .\n",
      "= 他 们 正 在 挖 一 个 洞 。\n",
      "< 他 们 正 在 聊 他 的 书 。 <EOS>\n",
      "\n",
      "> i m hoping that ll happen .\n",
      "= 我 希 望 那 会 发 生 。\n",
      "< 我 希 望 那 会 发 生 。 <EOS>\n",
      "\n",
      "> i m free tonight .\n",
      "= 我 今 晚 有 空 。\n",
      "< 我 今 晚 离 开 。 <EOS>\n",
      "\n",
      "> they are very big .\n",
      "= 他 们 非 常 大 。\n",
      "< 他 们 非 常 快 。 <EOS>\n",
      "\n",
      "> i am playing the piano now .\n",
      "= 我 现 在 正 在 弹 钢 琴 。\n",
      "< 我 现 在 正 在 弹 钢 琴 。 <EOS>\n",
      "\n",
      "> i m not too smart .\n",
      "= 我 不 太 聪 明 。\n",
      "< 我 不 太 聪 明 。 <EOS>\n",
      "\n",
      "> i m married .\n",
      "= 我 已 婚 。\n",
      "< 我 已 婚 。 <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_randomly(encoder, decoder, n=10):\n",
    "    # 随机选择几个句子查看学习的效果\n",
    "    reset_seed()\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = inference(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words[1:])\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "evaluate_randomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = i m ok .\n",
      "output = <SOS> 我 没 事 。 <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = he s lazy .\n",
      "output = <SOS> 他 很 懒 。 <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = i m sorry .\n",
      "output = <SOS> 我 在 做 不 起 。 <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = i m lazy .\n",
      "output = <SOS> 我 很 好 。 <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用matplotlib进行可视化，纵坐标为目标的输入词，横坐标为源语言句子\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence):\n",
    "    output_words, attentions = inference(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    show_attention(input_sentence, output_words, attentions)\n",
    "\n",
    "pairs = [[en, zh] for en, zh in zip(ens, zhs)]\n",
    "evaluate_and_show_attention(pairs[0][0])\n",
    "evaluate_and_show_attention(pairs[10][0])\n",
    "evaluate_and_show_attention(pairs[20][0])\n",
    "evaluate_and_show_attention('i m lazy .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**BPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def show_bold(string):\n",
    "    display(Markdown(f'**{string}**'))\n",
    "\n",
    "def show_heading(string, level=4):\n",
    "    display(Markdown('#'*level + ' ' + string))\n",
    "\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Iteration 1**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('e', 's')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 2**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('es', 't')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 3**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('est', '</w>')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 4**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('l', 'o')\n",
      "train data: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 5**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lo', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 6**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('n', 'e')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 7**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('ne', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 8**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('new', 'est</w>')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 9**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('low', '</w>')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 10**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('w', 'i')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "train_data = {\n",
    "    'l o w </w>': 5,\n",
    "    'l o w e r </w>': 2,\n",
    "    'n e w e s t </w>': 6,\n",
    "    'w i d e s t </w>': 3\n",
    "}\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    show_bold(f\"Iteration {i+1}\")\n",
    "    pairs = get_stats(train_data)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    train_data = merge_vocab(best, train_data)\n",
    "\n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "\n",
    "    print(\"new merge: {}\".format(best))\n",
    "    print(\"train data: {}\".format(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def encode(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    print(\"word split into characters: {}\".format(word))\n",
    "\n",
    "    pairs = get_pairs(word)\n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        show_bold(f\"Iteration {iteration}:\")\n",
    "\n",
    "        print(f\"bigrams in the word: {pairs}\")\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        print(f\"candidate for merging: {bigram}\")\n",
    "        if bigram not in bpe_codes:\n",
    "            print(\"Candidate not in BPE merges, algorithm stops.\")\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(f\"word after merging: {word}\")\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word split into characters: ('l', 'o', 'w', 'e', 's', 't', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('t', '</w>'), ('w', 'e'), ('s', 't'), ('e', 's'), ('o', 'w'), ('l', 'o')}\n",
      "candidate for merging: ('e', 's')\n",
      "word after merging: ('l', 'o', 'w', 'es', 't', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 2:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('t', '</w>'), ('o', 'w'), ('w', 'es'), ('l', 'o'), ('es', 't')}\n",
      "candidate for merging: ('es', 't')\n",
      "word after merging: ('l', 'o', 'w', 'est', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 3:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('est', '</w>'), ('l', 'o'), ('o', 'w'), ('w', 'est')}\n",
      "candidate for merging: ('est', '</w>')\n",
      "word after merging: ('l', 'o', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 4:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('w', 'est</w>'), ('l', 'o'), ('o', 'w')}\n",
      "candidate for merging: ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 5:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('w', 'est</w>'), ('lo', 'w')}\n",
      "candidate for merging: ('lo', 'w')\n",
      "word after merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Iteration 6:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('low', 'est</w>')}\n",
      "candidate for merging: ('low', 'est</w>')\n",
      "Candidate not in BPE merges, algorithm stops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**BERT模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 213450/213450 [00:01<00:00, 204211.11B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 995526/995526 [00:01<00:00, 552053.84B/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 109540/109540 [00:01<00:00, 76834.48B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 815973/815973 [00:02<00:00, 380026.92B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 458495/458495 [00:01<00:00, 276237.86B/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1042301/1042301 [00:02<00:00, 496013.91B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 456318/456318 [00:01<00:00, 257856.62B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 9143613/9143613 [00:13<00:00, 664070.06B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 798011/798011 [00:02<00:00, 367891.37B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 646181/646181 [00:02<00:00, 319981.66B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 486639/486639 [00:01<00:00, 266871.60B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 2952532/2952532 [00:06<00:00, 488878.36B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1434601/1434601 [00:02<00:00, 570278.61B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 898823/898823 [00:02<00:00, 415427.96B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 456318/456318 [00:01<00:00, 249499.41B/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (BertTokenizer, OpenAIGPTTokenizer,\n",
    "    TransfoXLTokenizer, GPT2Tokenizer,  XLMTokenizer, XLNetTokenizer, RobertaTokenizer)\n",
    "\n",
    "\n",
    "tokenizers ={\n",
    "    'bert-base-cased': BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "    'bert-base-multilingual-cased': BertTokenizer.from_pretrained('bert-base-multilingual-cased'),\n",
    "    'bert-base-chinese': BertTokenizer.from_pretrained('bert-base-chinese'),\n",
    "    'gpt': OpenAIGPTTokenizer.from_pretrained('openai-gpt'),\n",
    "    'gpt2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    'transformerxl': TransfoXLTokenizer.from_pretrained('transfo-xl-wt103'),\n",
    "    'xlnet-base-cased':  XLNetTokenizer.from_pretrained('xlnet-base-cased'),\n",
    "    'xlm-mlm-en': XLMTokenizer.from_pretrained('xlm-mlm-en-2048'),\n",
    "    'xlm-mlm-tlm-xnli15-1024': XLMTokenizer.from_pretrained('xlm-mlm-tlm-xnli15-1024'),\n",
    "    'roberta-base': RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformers_tokenize_sentence(sentence):\n",
    "    print(f'sentence:\\n{sentence}')\n",
    "    print()\n",
    "    for name, tok in tokenizers.items():\n",
    "        print(f'{name}:\\n{tok.tokenize(sentence)}')\n",
    "        print()\n",
    "        print(f'{name} (recover):\\n{tok.decode(tok.encode(sentence))}')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.',\n",
    "    '独角兽是一种传说生物，形象通常为头上长有独角的白马。现实世界中，独角鲸也许是这个神话动物的原型，不过独角鲸生活在遥远的北冰洋的深海而不是山川草原。生活在陆地上的犀牛，前额也有一只尖利的角，可惜相貌差距甚远。也有人说，独角兽其实就是已灭绝的板齿犀，亦称为西伯利亚独角兽，大约在2.9万年前灭绝。',\n",
    "    'El unicornio es una criatura mitológica representada habitualmente como un caballo blanco con patas de antílope, ojos y barba de chivo y un cuerno en la frente.',\n",
    "    \"La licorne, parfois nommée unicorne, est une créature légendaire à corne unique. Son origine, controversée, résulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinocéros et l'antilope, issues de récits d'explorateurs.\",\n",
    "    'Das Einhorn (lat. unicornis, griech. monókeros) ist ein Fabelwesen von Pferde- oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte.',\n",
    "    'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.',\n",
    "    'We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.',\n",
    "    'こんにちは世界。',\n",
    "    '你好，世界。',\n",
    "    'Hello, world.',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Sentence 0**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "bert-base-cased:\n",
      "['In', 'a', 'shocking', 'finding', ',', 'scientist', 'discovered', 'a', 'herd', 'of', 'un', '##icorn', '##s', 'living', 'in', 'a', 'remote', ',', 'previously', 'une', '##x', '##p', '##lore', '##d', 'valley', ',', 'in', 'the', 'Andes', 'Mountains', '.', 'Even', 'more', 'surprising', 'to', 'the', 'researchers', 'was', 'the', 'fact', 'that', 'the', 'un', '##icorn', '##s', 'spoke', 'perfect', 'English', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['In', 'a', 'shock', '##ing', 'finding', ',', 'scientist', 'discovered', 'a', 'her', '##d', 'of', 'unico', '##rns', 'living', 'in', 'a', 'remote', ',', 'previously', 'une', '##x', '##plo', '##red', 'valley', ',', 'in', 'the', 'Andes', 'Mountains', '.', 'Even', 'more', 'sur', '##pris', '##ing', 'to', 'the', 'researchers', 'was', 'the', 'fact', 'that', 'the', 'unico', '##rns', 'spoke', 'perfect', 'English', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', 'a', 'sh', '##ock', '##ing', 'find', '##ing', ',', 'sci', '##ent', '##ist', 'di', '##sco', '##ver', '##ed', 'a', 'her', '##d', 'of', 'u', '##nic', '##or', '##ns', 'living', 'in', 'a', 're', '##mo', '##te', ',', 'previous', '##ly', 'u', '##ne', '##xp', '##lor', '##ed', 'valley', ',', 'in', 'the', '[UNK]', '[UNK]', '.', '[UNK]', 'more', 'su', '##rp', '##ris', '##ing', 'to', 'the', 'research', '##ers', 'was', 'the', 'fa', '##ct', 'that', 'the', 'u', '##nic', '##or', '##ns', 'sp', '##ok', '##e', 'pe', '##rf', '##ect', '[UNK]', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK] a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the [UNK] [UNK]. [UNK] more surprising to the researchers was the fact that the unicorns spoke perfect [UNK]. [SEP]\n",
      "\n",
      "gpt:\n",
      "['in</w>', 'a</w>', 'shocking</w>', 'finding</w>', ',</w>', 'scientist</w>', 'discovered</w>', 'a</w>', 'herd</w>', 'of</w>', 'unicorns</w>', 'living</w>', 'in</w>', 'a</w>', 'remote</w>', ',</w>', 'previously</w>', 'unex', 'pl', 'ored</w>', 'valley</w>', ',</w>', 'in</w>', 'the</w>', 'and', 'es</w>', 'mountains</w>', '.</w>', 'even</w>', 'more</w>', 'surprising</w>', 'to</w>', 'the</w>', 'researchers</w>', 'was</w>', 'the</w>', 'fact</w>', 'that</w>', 'the</w>', 'unicorns</w>', 'spoke</w>', 'perfect</w>', 'english</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english.\n",
      "\n",
      "gpt2:\n",
      "['In', 'Ġa', 'Ġshocking', 'Ġfinding', ',', 'Ġscientist', 'Ġdiscovered', 'Ġa', 'Ġherd', 'Ġof', 'Ġunic', 'orns', 'Ġliving', 'Ġin', 'Ġa', 'Ġremote', ',', 'Ġpreviously', 'Ġunexpl', 'ored', 'Ġvalley', ',', 'Ġin', 'Ġthe', 'ĠAnd', 'es', 'ĠMountains', '.', 'ĠEven', 'Ġmore', 'Ġsurprising', 'Ġto', 'Ġthe', 'Ġresearchers', 'Ġwas', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġthe', 'Ġunic', 'orns', 'Ġspoke', 'Ġperfect', 'ĠEnglish', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "transformerxl:\n",
      "['In', 'a', 'shocking', 'finding,', 'scientist', 'discovered', 'a', 'herd', 'of', 'unicorns', 'living', 'in', 'a', 'remote,', 'previously', 'unexplored', 'valley,', 'in', 'the', 'Andes', 'Mountains.', 'Even', 'more', 'surprising', 'to', 'the', 'researchers', 'was', 'the', 'fact', 'that', 'the', 'unicorns', 'spoke', 'perfect', 'English.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "In a shocking <unk> scientist discovered a herd of unicorns living in a <unk> previously unexplored <unk> in the Andes <unk> Even more surprising to the researchers was the fact that the unicorns spoke perfect <unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁In', '▁a', '▁shocking', '▁finding', ',', '▁scientist', '▁discovered', '▁a', '▁her', 'd', '▁of', '▁', 'uni', 'corn', 's', '▁living', '▁in', '▁a', '▁remote', ',', '▁previously', '▁un', 'exp', 'lor', 'ed', '▁valley', ',', '▁in', '▁the', '▁And', 'es', '▁Mountains', '.', '▁Even', '▁more', '▁surprising', '▁to', '▁the', '▁researchers', '▁was', '▁the', '▁fact', '▁that', '▁the', '▁', 'uni', 'corn', 's', '▁spoke', '▁perfect', '▁English', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['in</w>', 'a</w>', 'shocking</w>', 'finding</w>', ',</w>', 'scientist</w>', 'discovered</w>', 'a</w>', 'herd</w>', 'of</w>', 'unic', 'orns</w>', 'living</w>', 'in</w>', 'a</w>', 'remote</w>', ',</w>', 'previously</w>', 'un', 'explored</w>', 'valley</w>', ',</w>', 'in</w>', 'the</w>', 'andes</w>', 'mountains</w>', '.</w>', 'even</w>', 'more</w>', 'surprising</w>', 'to</w>', 'the</w>', 'researchers</w>', 'was</w>', 'the</w>', 'fact</w>', 'that</w>', 'the</w>', 'unic', 'orns</w>', 'spoke</w>', 'perfect</w>', 'english</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['in</w>', 'a</w>', 'sho', 'cking</w>', 'finding</w>', ',</w>', 'sci', 'ent', 'ist</w>', 'discovered</w>', 'a</w>', 'her', 'd</w>', 'of</w>', 'unic', 'or', 'ns</w>', 'living</w>', 'in</w>', 'a</w>', 'remo', 'te</w>', ',</w>', 'previ', 'ously</w>', 'un', 'explo', 'red</w>', 'valley</w>', ',</w>', 'in</w>', 'the</w>', 'andes</w>', 'mountains</w>', '.</w>', 'even</w>', 'more</w>', 'surpris', 'ing</w>', 'to</w>', 'the</w>', 'rese', 'arch', 'ers</w>', 'was</w>', 'the</w>', 'fact</w>', 'that</w>', 'the</w>', 'unic', 'or', 'ns</w>', 'spoke</w>', 'perfect</w>', 'english</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. </s>\n",
      "\n",
      "roberta-base:\n",
      "['In', 'Ġa', 'Ġshocking', 'Ġfinding', ',', 'Ġscientist', 'Ġdiscovered', 'Ġa', 'Ġherd', 'Ġof', 'Ġunic', 'orns', 'Ġliving', 'Ġin', 'Ġa', 'Ġremote', ',', 'Ġpreviously', 'Ġunexpl', 'ored', 'Ġvalley', ',', 'Ġin', 'Ġthe', 'ĠAnd', 'es', 'ĠMountains', '.', 'ĠEven', 'Ġmore', 'Ġsurprising', 'Ġto', 'Ġthe', 'Ġresearchers', 'Ġwas', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġthe', 'Ġunic', 'orns', 'Ġspoke', 'Ġperfect', 'ĠEnglish', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 1**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "独角兽是一种传说生物，形象通常为头上长有独角的白马。现实世界中，独角鲸也许是这个神话动物的原型，不过独角鲸生活在遥远的北冰洋的深海而不是山川草原。生活在陆地上的犀牛，前额也有一只尖利的角，可惜相貌差距甚远。也有人说，独角兽其实就是已灭绝的板齿犀，亦称为西伯利亚独角兽，大约在2.9万年前灭绝。\n",
      "\n",
      "bert-base-cased:\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '一', '[UNK]', '[UNK]', '[UNK]', '生', '[UNK]', '，', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '上', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '白', '[UNK]', '。', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '中', '，', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '神', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '原', '[UNK]', '，', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '生', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '北', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '海', '[UNK]', '[UNK]', '[UNK]', '山', '川', '[UNK]', '原', '。', '生', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '上', '[UNK]', '[UNK]', '[UNK]', '，', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '一', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '，', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '。', '[UNK]', '[UNK]', '人', '[UNK]', '，', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '，', '[UNK]', '[UNK]', '[UNK]', '西', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '，', '大', '[UNK]', '[UNK]', '2', '.', '9', '[UNK]', '年', '[UNK]', '[UNK]', '[UNK]', '。']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] [UNK] [UNK] [UNK] [UNK] 一 [UNK] [UNK] [UNK] 生 [UNK] ， [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 上 [UNK] [UNK] [UNK] [UNK] [UNK] 白 [UNK] 。 [UNK] [UNK] [UNK] [UNK] 中 ， [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 神 [UNK] [UNK] [UNK] [UNK] 原 [UNK] ， [UNK] [UNK] [UNK] [UNK] [UNK] 生 [UNK] [UNK] [UNK] [UNK] [UNK] 北 [UNK] [UNK] [UNK] [UNK] 海 [UNK] [UNK] [UNK] 山 川 [UNK] 原 。 生 [UNK] [UNK] [UNK] [UNK] 上 [UNK] [UNK] [UNK] ， [UNK] [UNK] [UNK] [UNK] 一 [UNK] [UNK] [UNK] [UNK] [UNK] ， [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 。 [UNK] [UNK] 人 [UNK] ， [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ， [UNK] [UNK] [UNK] 西 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ， 大 [UNK] [UNK] 2. 9 [UNK] 年 [UNK] [UNK] [UNK] 。 [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['独', '角', '兽', '是', '一', '种', '传', '说', '生', '物', '，', '形', '象', '通', '常', '为', '头', '上', '长', '有', '独', '角', '的', '白', '马', '。', '现', '实', '世', '界', '中', '，', '独', '角', '鲸', '也', '许', '是', '这', '个', '神', '话', '动', '物', '的', '原', '型', '，', '不', '过', '独', '角', '鲸', '生', '活', '在', '遥', '远', '的', '北', '冰', '洋', '的', '深', '海', '而', '不', '是', '山', '川', '草', '原', '。', '生', '活', '在', '陆', '地', '上', '的', '犀', '牛', '，', '前', '额', '也', '有', '一', '只', '尖', '利', '的', '角', '，', '可', '惜', '相', '貌', '差', '距', '甚', '远', '。', '也', '有', '人', '说', '，', '独', '角', '兽', '其', '实', '就', '是', '已', '灭', '绝', '的', '板', '齿', '犀', '，', '亦', '称', '为', '西', '伯', '利', '亚', '独', '角', '兽', '，', '大', '约', '在', '2', '.', '9', '万', '年', '前', '灭', '绝', '。']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] 独 角 兽 是 一 种 传 说 生 物 ， 形 象 通 常 为 头 上 长 有 独 角 的 白 马 。 现 实 世 界 中 ， 独 角 鲸 也 许 是 这 个 神 话 动 物 的 原 型 ， 不 过 独 角 鲸 生 活 在 遥 远 的 北 冰 洋 的 深 海 而 不 是 山 川 草 原 。 生 活 在 陆 地 上 的 犀 牛 ， 前 额 也 有 一 只 尖 利 的 角 ， 可 惜 相 貌 差 距 甚 远 。 也 有 人 说 ， 独 角 兽 其 实 就 是 已 灭 绝 的 板 齿 犀 ， 亦 称 为 西 伯 利 亚 独 角 兽 ， 大 约 在 2. 9 万 年 前 灭 绝 。 [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['独', '角', '兽', '是', '一', '种', '传', '说', '生', '物', '，', '形', '象', '通', '常', '为', '头', '上', '长', '有', '独', '角', '的', '白', '马', '。', '现', '实', '世', '界', '中', '，', '独', '角', '鲸', '也', '许', '是', '这', '个', '神', '话', '动', '物', '的', '原', '型', '，', '不', '过', '独', '角', '鲸', '生', '活', '在', '遥', '远', '的', '北', '冰', '洋', '的', '深', '海', '而', '不', '是', '山', '川', '草', '原', '。', '生', '活', '在', '陆', '地', '上', '的', '犀', '牛', '，', '前', '额', '也', '有', '一', '只', '尖', '利', '的', '角', '，', '可', '惜', '相', '貌', '差', '距', '甚', '远', '。', '也', '有', '人', '说', '，', '独', '角', '兽', '其', '实', '就', '是', '已', '灭', '绝', '的', '板', '齿', '犀', '，', '亦', '称', '为', '西', '伯', '利', '亚', '独', '角', '兽', '，', '大', '约', '在', '2', '.', '9', '万', '年', '前', '灭', '绝', '。']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] 独 角 兽 是 一 种 传 说 生 物 ， 形 象 通 常 为 头 上 长 有 独 角 的 白 马 。 现 实 世 界 中 ， 独 角 鲸 也 许 是 这 个 神 话 动 物 的 原 型 ， 不 过 独 角 鲸 生 活 在 遥 远 的 北 冰 洋 的 深 海 而 不 是 山 川 草 原 。 生 活 在 陆 地 上 的 犀 牛 ， 前 额 也 有 一 只 尖 利 的 角 ， 可 惜 相 貌 差 距 甚 远 。 也 有 人 说 ， 独 角 兽 其 实 就 是 已 灭 绝 的 板 齿 犀 ， 亦 称 为 西 伯 利 亚 独 角 兽 ， 大 约 在 2. 9 万 年 前 灭 绝 。 [SEP]\n",
      "\n",
      "gpt:\n",
      "['独</w>', '角</w>', '兽</w>', '是</w>', '一</w>', '种</w>', '传</w>', '说</w>', '生</w>', '物</w>', '，</w>', '形</w>', '象</w>', '通</w>', '常</w>', '为</w>', '头</w>', '上</w>', '长</w>', '有</w>', '独</w>', '角</w>', '的</w>', '白</w>', '马</w>', '。</w>', '现</w>', '实</w>', '世</w>', '界</w>', '中</w>', '，</w>', '独</w>', '角</w>', '鲸</w>', '也</w>', '许</w>', '是</w>', '这</w>', '个</w>', '神</w>', '话</w>', '动</w>', '物</w>', '的</w>', '原</w>', '型</w>', '，</w>', '不</w>', '过</w>', '独</w>', '角</w>', '鲸</w>', '生</w>', '活</w>', '在</w>', '遥</w>', '远</w>', '的</w>', '北</w>', '冰</w>', '洋</w>', '的</w>', '深</w>', '海</w>', '而</w>', '不</w>', '是</w>', '山</w>', '川</w>', '草</w>', '原</w>', '。</w>', '生</w>', '活</w>', '在</w>', '陆</w>', '地</w>', '上</w>', '的</w>', '犀</w>', '牛</w>', '，</w>', '前</w>', '额</w>', '也</w>', '有</w>', '一</w>', '只</w>', '尖</w>', '利</w>', '的</w>', '角</w>', '，</w>', '可</w>', '惜</w>', '相</w>', '貌</w>', '差</w>', '距</w>', '甚</w>', '远</w>', '。</w>', '也</w>', '有</w>', '人</w>', '说</w>', '，</w>', '独</w>', '角</w>', '兽</w>', '其</w>', '实</w>', '就</w>', '是</w>', '已</w>', '灭</w>', '绝</w>', '的</w>', '板</w>', '齿</w>', '犀</w>', '，</w>', '亦</w>', '称</w>', '为</w>', '西</w>', '伯</w>', '利</w>', '亚</w>', '独</w>', '角</w>', '兽</w>', '，</w>', '大</w>', '约</w>', '在</w>', '2</w>', '.</w>', '9</w>', '万</w>', '年</w>', '前</w>', '灭</w>', '绝</w>', '。</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>2. 9 <unk><unk><unk><unk><unk><unk>\n",
      "\n",
      "gpt2:\n",
      "['çĭ', '¬', 'è', '§', 'Ĵ', 'åħ', '½', 'æĺ¯', 'ä¸Ģ', 'ç', '§', 'į', 'ä¼', 'ł', 'è¯', '´', 'çĶŁ', 'çī', '©', 'ï', '¼', 'Į', 'å½', '¢', 'è', '±', '¡', 'éĢ', 'ļ', 'å¸', '¸', 'ä¸', 'º', 'å¤', '´', 'ä¸Ĭ', 'é', 'ķ', '¿', 'æľ', 'ī', 'çĭ', '¬', 'è', '§', 'Ĵ', 'çļĦ', 'ç', 'Ļ½', 'é', '©', '¬', 'ãĢĤ', 'ç', 'İ', '°', 'å®', 'ŀ', 'ä¸', 'ĸ', 'çķ', 'Į', 'ä¸Ń', 'ï', '¼', 'Į', 'çĭ', '¬', 'è', '§', 'Ĵ', 'é', '²', '¸', 'ä¹', 'Ł', 'è', '®', '¸', 'æĺ¯', 'è¿', 'Ļ', 'ä¸', 'ª', 'ç¥ŀ', 'è¯', 'Ŀ', 'åĬ', '¨', 'çī', '©', 'çļĦ', 'åİ', 'Ł', 'å', 'ŀ', 'ĭ', 'ï', '¼', 'Į', 'ä¸į', 'è¿', 'ĩ', 'çĭ', '¬', 'è', '§', 'Ĵ', 'é', '²', '¸', 'çĶŁ', 'æ', '´', '»', 'åľ', '¨', 'éģ', '¥', 'è¿', 'ľ', 'çļĦ', 'åĮ', 'Ĺ', 'åĨ', '°', 'æ', '´', 'ĭ', 'çļĦ', 'æ', '·', '±', 'æµ', '·', 'èĢ', 'Į', 'ä¸į', 'æĺ¯', 'å', '±', '±', 'å·', 'Ŀ', 'è', 'į', 'ī', 'åİ', 'Ł', 'ãĢĤ', 'çĶŁ', 'æ', '´', '»', 'åľ', '¨', 'é', 'Ļ', 'Ĩ', 'åľ', '°', 'ä¸Ĭ', 'çļĦ', 'ç', 'Ĭ', 'Ģ', 'çī', 'Ľ', 'ï', '¼', 'Į', 'åī', 'į', 'é', '¢', 'Ŀ', 'ä¹', 'Ł', 'æľ', 'ī', 'ä¸Ģ', 'åı', 'ª', 'å°', 'ĸ', 'åĪ', '©', 'çļĦ', 'è', '§', 'Ĵ', 'ï', '¼', 'Į', 'åı', '¯', 'æĥ', 'ľ', 'çĽ', '¸', 'è', '²', 'Į', 'å·', '®', 'è', '·', 'Ŀ', 'çĶ', 'ļ', 'è¿', 'ľ', 'ãĢĤ', 'ä¹', 'Ł', 'æľ', 'ī', 'äºº', 'è¯', '´', 'ï', '¼', 'Į', 'çĭ', '¬', 'è', '§', 'Ĵ', 'åħ', '½', 'åħ', '¶', 'å®', 'ŀ', 'å°', '±', 'æĺ¯', 'å·', '²', 'ç', 'ģ', 'Ń', 'ç', '»', 'Ŀ', 'çļĦ', 'æĿ', '¿', 'é', '½', '¿', 'ç', 'Ĭ', 'Ģ', 'ï', '¼', 'Į', 'äº', '¦', 'ç', '§', '°', 'ä¸', 'º', 'è', '¥', '¿', 'ä¼', '¯', 'åĪ', '©', 'äº', 'ļ', 'çĭ', '¬', 'è', '§', 'Ĵ', 'åħ', '½', 'ï', '¼', 'Į', 'å¤§', 'ç', 'º', '¦', 'åľ', '¨', '2', '.', '9', 'ä¸', 'ĩ', 'å¹', '´', 'åī', 'į', 'ç', 'ģ', 'Ń', 'ç', '»', 'Ŀ', 'ãĢĤ']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "独角兽是一种传说生物，形象通常为头上长有独角的白马。现实世界中，独角鲸也许是这个神话动物的原型，不过独角鲸生活在遥远的北冰洋的深海而不是山川草原。生活在陆地上的犀牛，前额也有一只尖利的角，可惜相貌差距甚远。也有人说，独角兽其实就是已灭绝的板齿犀，亦称为西伯利亚独角兽，大约在2.9万年前灭绝。\n",
      "\n",
      "transformerxl:\n",
      "['独角兽是一种传说生物，形象通常为头上长有独角的白马。现实世界中，独角鲸也许是这个神话动物的原型，不过独角鲸生活在遥远的北冰洋的深海而不是山川草原。生活在陆地上的犀牛，前额也有一只尖利的角，可惜相貌差距甚远。也有人说，独角兽其实就是已灭绝的板齿犀，亦称为西伯利亚独角兽，大约在2.9万年前灭绝。']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "<unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁', '独角兽是一种传说生物', ',', '形象通常为头上长有独角的白马。现实世界中', ',', '独角鲸也许是这个神话动物的原型', ',', '不过独角鲸生活在遥远的北冰洋的深海而不是山川草原。生活在陆地上的犀牛', ',', '前额也有一只尖利的角', ',', '可惜相貌差距甚远。也有人说', ',', '独角兽其实就是已灭绝的板齿犀', ',', '亦称为西伯利亚独角兽', ',', '大约在', '2', '.', '9', '万年前灭绝。']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>2.9<unk><sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['独</w>', '角</w>', '兽</w>', '是</w>', '一</w>', '种</w>', '传</w>', '说</w>', '生</w>', '物</w>', ',</w>', '形</w>', '象</w>', '通</w>', '常</w>', '为</w>', '头</w>', '上</w>', '长</w>', '有</w>', '独</w>', '角</w>', '的</w>', '白</w>', '马</w>', '.</w>', '现</w>', '实</w>', '世</w>', '界</w>', '中</w>', ',</w>', '独</w>', '角</w>', '鲸</w>', '也</w>', '许</w>', '是</w>', '这</w>', '个</w>', '神</w>', '话</w>', '动</w>', '物</w>', '的</w>', '原</w>', '型</w>', ',</w>', '不</w>', '过</w>', '独</w>', '角</w>', '鲸</w>', '生</w>', '活</w>', '在</w>', '遥</w>', '远</w>', '的</w>', '北</w>', '冰</w>', '洋</w>', '的</w>', '深</w>', '海</w>', '而</w>', '不</w>', '是</w>', '山</w>', '川</w>', '草</w>', '原</w>', '.</w>', '生</w>', '活</w>', '在</w>', '陆</w>', '地</w>', '上</w>', '的</w>', '犀</w>', '牛</w>', ',</w>', '前</w>', '额</w>', '也</w>', '有</w>', '一</w>', '只</w>', '尖</w>', '利</w>', '的</w>', '角</w>', ',</w>', '可</w>', '惜</w>', '相</w>', '貌</w>', '差</w>', '距</w>', '甚</w>', '远</w>', '.</w>', '也</w>', '有</w>', '人</w>', '说</w>', ',</w>', '独</w>', '角</w>', '兽</w>', '其</w>', '实</w>', '就</w>', '是</w>', '已</w>', '灭</w>', '绝</w>', '的</w>', '板</w>', '齿</w>', '犀</w>', ',</w>', '亦</w>', '称</w>', '为</w>', '西</w>', '伯</w>', '利</w>', '亚</w>', '独</w>', '角</w>', '兽</w>', ',</w>', '大</w>', '约</w>', '在</w>', '2.9</w>', '万</w>', '年</w>', '前</w>', '灭</w>', '绝</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>. <unk><unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>. <unk><unk><unk><unk><unk><unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk>. <unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>, <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>, <unk><unk><unk>2.9 <unk><unk><unk><unk><unk>. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['独</w>', '角</w>', '兽</w>', '是</w>', '一</w>', '种</w>', '传</w>', '说</w>', '生</w>', '物</w>', ',</w>', '形</w>', '象</w>', '通</w>', '常</w>', '为</w>', '头</w>', '上</w>', '长</w>', '有</w>', '独</w>', '角</w>', '的</w>', '白</w>', '马</w>', '.</w>', '现</w>', '实</w>', '世</w>', '界</w>', '中</w>', ',</w>', '独</w>', '角</w>', '鲸</w>', '也</w>', '许</w>', '是</w>', '这</w>', '个</w>', '神</w>', '话</w>', '动</w>', '物</w>', '的</w>', '原</w>', '型</w>', ',</w>', '不</w>', '过</w>', '独</w>', '角</w>', '鲸</w>', '生</w>', '活</w>', '在</w>', '遥</w>', '远</w>', '的</w>', '北</w>', '冰</w>', '洋</w>', '的</w>', '深</w>', '海</w>', '而</w>', '不</w>', '是</w>', '山</w>', '川</w>', '草</w>', '原</w>', '.</w>', '生</w>', '活</w>', '在</w>', '陆</w>', '地</w>', '上</w>', '的</w>', '犀</w>', '牛</w>', ',</w>', '前</w>', '额</w>', '也</w>', '有</w>', '一</w>', '只</w>', '尖</w>', '利</w>', '的</w>', '角</w>', ',</w>', '可</w>', '惜</w>', '相</w>', '貌</w>', '差</w>', '距</w>', '甚</w>', '远</w>', '.</w>', '也</w>', '有</w>', '人</w>', '说</w>', ',</w>', '独</w>', '角</w>', '兽</w>', '其</w>', '实</w>', '就</w>', '是</w>', '已</w>', '灭</w>', '绝</w>', '的</w>', '板</w>', '齿</w>', '犀</w>', ',</w>', '亦</w>', '称</w>', '为</w>', '西</w>', '伯</w>', '利</w>', '亚</w>', '独</w>', '角</w>', '兽</w>', ',</w>', '大</w>', '约</w>', '在</w>', '2.9</w>', '万</w>', '年</w>', '前</w>', '灭</w>', '绝</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>独 角 兽 是 一 种 传 说 生 物, 形 象 通 常 为 头 上 长 有 独 角 的 白 马. 现 实 世 界 中, 独 角 鲸 也 许 是 这 个 神 话 动 物 的 原 型, 不 过 独 角 鲸 生 活 在 遥 远 的 北 冰 洋 的 深 海 而 不 是 山 川 草 原. 生 活 在 陆 地 上 的 犀 牛, 前 额 也 有 一 只 尖 利 的 角, 可 惜 相 貌 差 距 甚 远. 也 有 人 说, 独 角 兽 其 实 就 是 已 灭 绝 的 板 齿 犀, 亦 称 为 西 伯 利 亚 独 角 兽, 大 约 在 2.9 万 年 前 灭 绝. </s>\n",
      "\n",
      "roberta-base:\n",
      "['çĭ', '¬', 'è', '§', 'Ĵ', 'åħ', '½', 'æĺ¯', 'ä¸Ģ', 'ç', '§', 'į', 'ä¼', 'ł', 'è¯', '´', 'çĶŁ', 'çī', '©', 'ï', '¼', 'Į', 'å½', '¢', 'è', '±', '¡', 'éĢ', 'ļ', 'å¸', '¸', 'ä¸', 'º', 'å¤', '´', 'ä¸Ĭ', 'é', 'ķ', '¿', 'æľ', 'ī', 'çĭ', '¬', 'è', '§', 'Ĵ', 'çļĦ', 'ç', 'Ļ½', 'é', '©', '¬', 'ãĢĤ', 'ç', 'İ', '°', 'å®', 'ŀ', 'ä¸', 'ĸ', 'çķ', 'Į', 'ä¸Ń', 'ï', '¼', 'Į', 'çĭ', '¬', 'è', '§', 'Ĵ', 'é', '²', '¸', 'ä¹', 'Ł', 'è', '®', '¸', 'æĺ¯', 'è¿', 'Ļ', 'ä¸', 'ª', 'ç¥ŀ', 'è¯', 'Ŀ', 'åĬ', '¨', 'çī', '©', 'çļĦ', 'åİ', 'Ł', 'å', 'ŀ', 'ĭ', 'ï', '¼', 'Į', 'ä¸į', 'è¿', 'ĩ', 'çĭ', '¬', 'è', '§', 'Ĵ', 'é', '²', '¸', 'çĶŁ', 'æ', '´', '»', 'åľ', '¨', 'éģ', '¥', 'è¿', 'ľ', 'çļĦ', 'åĮ', 'Ĺ', 'åĨ', '°', 'æ', '´', 'ĭ', 'çļĦ', 'æ', '·', '±', 'æµ', '·', 'èĢ', 'Į', 'ä¸į', 'æĺ¯', 'å', '±', '±', 'å·', 'Ŀ', 'è', 'į', 'ī', 'åİ', 'Ł', 'ãĢĤ', 'çĶŁ', 'æ', '´', '»', 'åľ', '¨', 'é', 'Ļ', 'Ĩ', 'åľ', '°', 'ä¸Ĭ', 'çļĦ', 'ç', 'Ĭ', 'Ģ', 'çī', 'Ľ', 'ï', '¼', 'Į', 'åī', 'į', 'é', '¢', 'Ŀ', 'ä¹', 'Ł', 'æľ', 'ī', 'ä¸Ģ', 'åı', 'ª', 'å°', 'ĸ', 'åĪ', '©', 'çļĦ', 'è', '§', 'Ĵ', 'ï', '¼', 'Į', 'åı', '¯', 'æĥ', 'ľ', 'çĽ', '¸', 'è', '²', 'Į', 'å·', '®', 'è', '·', 'Ŀ', 'çĶ', 'ļ', 'è¿', 'ľ', 'ãĢĤ', 'ä¹', 'Ł', 'æľ', 'ī', 'äºº', 'è¯', '´', 'ï', '¼', 'Į', 'çĭ', '¬', 'è', '§', 'Ĵ', 'åħ', '½', 'åħ', '¶', 'å®', 'ŀ', 'å°', '±', 'æĺ¯', 'å·', '²', 'ç', 'ģ', 'Ń', 'ç', '»', 'Ŀ', 'çļĦ', 'æĿ', '¿', 'é', '½', '¿', 'ç', 'Ĭ', 'Ģ', 'ï', '¼', 'Į', 'äº', '¦', 'ç', '§', '°', 'ä¸', 'º', 'è', '¥', '¿', 'ä¼', '¯', 'åĪ', '©', 'äº', 'ļ', 'çĭ', '¬', 'è', '§', 'Ĵ', 'åħ', '½', 'ï', '¼', 'Į', 'å¤§', 'ç', 'º', '¦', 'åľ', '¨', '2', '.', '9', 'ä¸', 'ĩ', 'å¹', '´', 'åī', 'į', 'ç', 'ģ', 'Ń', 'ç', '»', 'Ŀ', 'ãĢĤ']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>独角兽是一种传说生物，形象通常为头上长有独角的白马。现实世界中，独角鲸也许是这个神话动物的原型，不过独角鲸生活在遥远的北冰洋的深海而不是山川草原。生活在陆地上的犀牛，前额也有一只尖利的角，可惜相貌差距甚远。也有人说，独角兽其实就是已灭绝的板齿犀，亦称为西伯利亚独角兽，大约在2.9万年前灭绝。</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 2**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "El unicornio es una criatura mitológica representada habitualmente como un caballo blanco con patas de antílope, ojos y barba de chivo y un cuerno en la frente.\n",
      "\n",
      "bert-base-cased:\n",
      "['El', 'un', '##icorn', '##io', 'es', 'un', '##a', 'c', '##ria', '##tura', 'mit', '##ol', '##ó', '##gic', '##a', 'represent', '##ada', 'habit', '##ual', '##ment', '##e', 'com', '##o', 'un', 'cab', '##allo', 'b', '##lan', '##co', 'con', 'pat', '##as', 'de', 'ant', '##í', '##lop', '##e', ',', 'o', '##jos', 'y', 'bar', '##ba', 'de', 'ch', '##ivo', 'y', 'un', 'cue', '##rno', 'en', 'la', 'f', '##rent', '##e', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] El unicornio es una criatura mitológica representada habitualmente como un caballo blanco con patas de antílope, ojos y barba de chivo y un cuerno en la frente. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['El', 'unico', '##rni', '##o', 'es', 'una', 'c', '##riat', '##ura', 'mito', '##lógica', 'representada', 'habitualmente', 'como', 'un', 'caballo', 'blanco', 'con', 'patas', 'de', 'ant', '##íl', '##ope', ',', 'ojos', 'y', 'bar', '##ba', 'de', 'chi', '##vo', 'y', 'un', 'cu', '##erno', 'en', 'la', 'frente', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] El unicornio es una criatura mitológica representada habitualmente como un caballo blanco con patas de antílope, ojos y barba de chivo y un cuerno en la frente. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', 'u', '##nic', '##or', '##ni', '##o', 'es', 'u', '##na', 'cr', '##ia', '##tu', '##ra', '[UNK]', 're', '##pr', '##ese', '##nt', '##ada', 'ha', '##bit', '##ual', '##ment', '##e', 'com', '##o', 'u', '##n', 'ca', '##ball', '##o', 'bl', '##an', '##co', 'con', 'pa', '##ta', '##s', 'de', '[UNK]', ',', 'o', '##jo', '##s', 'y', 'bar', '##ba', 'de', 'chi', '##vo', 'y', 'u', '##n', 'c', '##ue', '##rn', '##o', 'en', 'la', 'f', '##rent', '##e', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK] unicornio es una criatura [UNK] representada habitualmente como un caballo blanco con patas de [UNK], ojos y barba de chivo y un cuerno en la frente. [SEP]\n",
      "\n",
      "gpt:\n",
      "['el</w>', 'unic', 'or', 'ni', 'o</w>', 'es</w>', 'una</w>', 'cri', 'at', 'ura</w>', 'mit', 'o', 'log', 'ica</w>', 'represen', 'ta', 'da</w>', 'habitu', 'al', 'men', 'te</w>', 'como</w>', 'un</w>', 'cab', 'al', 'lo</w>', 'blanco</w>', 'con</w>', 'pat', 'as</w>', 'de</w>', 'anti', 'lo', 'pe</w>', ',</w>', 'o', 'jo', 's</w>', 'y</w>', 'bar', 'ba</w>', 'de</w>', 'chi', 'vo</w>', 'y</w>', 'un</w>', 'cu', 'er', 'no</w>', 'en</w>', 'la</w>', 'fren', 'te</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "el unicornio es una criatura mitologica representada habitualmente como un caballo blanco con patas de antilope, ojos y barba de chivo y un cuerno en la frente.\n",
      "\n",
      "gpt2:\n",
      "['El', 'Ġunicorn', 'io', 'Ġes', 'Ġun', 'a', 'Ġc', 'ri', 'atur', 'a', 'Ġmit', 'ol', 'Ã³', 'g', 'ica', 'Ġrepresent', 'ada', 'Ġhabitual', 'ment', 'e', 'Ġcom', 'o', 'Ġun', 'Ġcab', 'allo', 'Ġbl', 'anco', 'Ġcon', 'Ġpat', 'as', 'Ġde', 'Ġant', 'ÃŃ', 'l', 'ope', ',', 'Ġo', 'j', 'os', 'Ġy', 'Ġbar', 'ba', 'Ġde', 'Ġch', 'ivo', 'Ġy', 'Ġun', 'Ġcu', 'erno', 'Ġen', 'Ġla', 'Ġf', 'rent', 'e', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "El unicornio es una criatura mitológica representada habitualmente como un caballo blanco con patas de antílope, ojos y barba de chivo y un cuerno en la frente.\n",
      "\n",
      "transformerxl:\n",
      "['El', 'unicornio', 'es', 'una', 'criatura', 'mitológica', 'representada', 'habitualmente', 'como', 'un', 'caballo', 'blanco', 'con', 'patas', 'de', 'antílope,', 'ojos', 'y', 'barba', 'de', 'chivo', 'y', 'un', 'cuerno', 'en', 'la', 'frente.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "El <unk> es una <unk> <unk> <unk> <unk> como un caballo blanco con <unk> de <unk> ojos y <unk> de chivo y un <unk> en la <unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁El', '▁', 'uni', 'corn', 'io', '▁', 'es', '▁', 'una', '▁', 'cri', 'atur', 'a', '▁', 'mit', 'ologic', 'a', '▁represent', 'ada', '▁habitual', 'ment', 'e', '▁com', 'o', '▁un', '▁ca', 'ball', 'o', '▁', 'b', 'lan', 'co', '▁con', '▁pat', 'as', '▁de', '▁anti', 'lope', ',', '▁', 'o', 'jo', 's', '▁', 'y', '▁bar', 'ba', '▁de', '▁', 'chi', 'vo', '▁', 'y', '▁un', '▁', 'cu', 'er', 'no', '▁', 'en', '▁la', '▁fr', 'ente', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "El unicornio es una criatura mitologica representada habitualmente como un caballo blanco con patas de antilope, ojos y barba de chivo y un cuerno en la frente.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['el</w>', 'unic', 'orni', 'o</w>', 'es</w>', 'una</w>', 'cri', 'at', 'ura</w>', 'mit', 'o', 'log', 'ica</w>', 'represent', 'ada</w>', 'habit', 'u', 'al', 'ment', 'e</w>', 'como</w>', 'un</w>', 'cab', 'allo</w>', 'blanco</w>', 'con</w>', 'pat', 'as</w>', 'de</w>', 'anti', 'lo', 'pe</w>', ',</w>', 'o', 'jos</w>', 'y</w>', 'bar', 'ba</w>', 'de</w>', 'chi', 'vo</w>', 'y</w>', 'un</w>', 'cu', 'erno</w>', 'en</w>', 'la</w>', 'fren', 'te</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>el unicornio es una criatura mitologica representada habitualmente como un caballo blanco con patas de antilope, ojos y barba de chivo y un cuerno en la frente. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['el</w>', 'unic', 'orn', 'io</w>', 'es</w>', 'una</w>', 'cri', 'atura</w>', 'mit', 'ologica</w>', 'represent', 'ada</w>', 'habitu', 'almente</w>', 'como</w>', 'un</w>', 'caballo</w>', 'blanco</w>', 'con</w>', 'pat', 'as</w>', 'de</w>', 'antil', 'ope</w>', ',</w>', 'ojos</w>', 'y</w>', 'bar', 'ba</w>', 'de</w>', 'ch', 'ivo</w>', 'y</w>', 'un</w>', 'cuer', 'no</w>', 'en</w>', 'la</w>', 'frente</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>el unicornio es una criatura mitologica representada habitualmente como un caballo blanco con patas de antilope, ojos y barba de chivo y un cuerno en la frente. </s>\n",
      "\n",
      "roberta-base:\n",
      "['El', 'Ġunicorn', 'io', 'Ġes', 'Ġun', 'a', 'Ġc', 'ri', 'atur', 'a', 'Ġmit', 'ol', 'Ã³', 'g', 'ica', 'Ġrepresent', 'ada', 'Ġhabitual', 'ment', 'e', 'Ġcom', 'o', 'Ġun', 'Ġcab', 'allo', 'Ġbl', 'anco', 'Ġcon', 'Ġpat', 'as', 'Ġde', 'Ġant', 'ÃŃ', 'l', 'ope', ',', 'Ġo', 'j', 'os', 'Ġy', 'Ġbar', 'ba', 'Ġde', 'Ġch', 'ivo', 'Ġy', 'Ġun', 'Ġcu', 'erno', 'Ġen', 'Ġla', 'Ġf', 'rent', 'e', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>El unicornio es una criatura mitológica representada habitualmente como un caballo blanco con patas de antílope, ojos y barba de chivo y un cuerno en la frente.</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 3**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "La licorne, parfois nommée unicorne, est une créature légendaire à corne unique. Son origine, controversée, résulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinocéros et l'antilope, issues de récits d'explorateurs.\n",
      "\n",
      "bert-base-cased:\n",
      "['La', 'l', '##icorn', '##e', ',', 'par', '##fo', '##is', 'no', '##mm', '##ée', 'un', '##icorn', '##e', ',', 'est', 'une', 'c', '##ré', '##ature', 'l', '##é', '##gen', '##dai', '##re', 'à', 'corn', '##e', 'unique', '.', 'Son', 'origin', '##e', ',', 'con', '##tro', '##vers', '##ée', ',', 'r', '##és', '##ult', '##e', 'de', 'multiple', '##s', 'influences', ',', 'en', 'part', '##ic', '##uli', '##er', 'de', 'descriptions', 'd', \"'\", 'an', '##ima', '##ux', 'te', '##ls', 'que', 'le', 'r', '##hino', '##cé', '##ros', 'et', 'l', \"'\", 'anti', '##lop', '##e', ',', 'issues', 'de', 'r', '##é', '##ci', '##ts', 'd', \"'\", 'ex', '##p', '##lora', '##te', '##urs', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] La licorne, parfois nommée unicorne, est une créature légendaire à corne unique. Son origine, controversée, résulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinocéros et l'antilope, issues de récits d'explorateurs. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['La', 'li', '##corn', '##e', ',', 'parfois', 'nommée', 'unico', '##rne', ',', 'est', 'une', 'c', '##réat', '##ure', 'l', '##ége', '##nda', '##ire', 'à', 'corn', '##e', 'unique', '.', 'Son', 'origine', ',', 'contro', '##vers', '##ée', ',', 'r', '##és', '##ulte', 'de', 'multiples', 'influences', ',', 'en', 'particulier', 'de', 'descriptions', 'd', \"'\", 'animaux', 'tels', 'que', 'le', 'r', '##hino', '##cé', '##ros', 'et', 'l', \"'\", 'anti', '##lope', ',', 'issues', 'de', 'récit', '##s', 'd', \"'\", 'ex', '##plo', '##rateur', '##s', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] La licorne, parfois nommée unicorne, est une créature légendaire à corne unique. Son origine, controversée, résulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinocéros et l'antilope, issues de récits d'explorateurs. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', 'li', '##co', '##rn', '##e', ',', 'pa', '##rf', '##oi', '##s', '[UNK]', 'u', '##nic', '##or', '##ne', ',', 'es', '##t', 'u', '##ne', '[UNK]', '[UNK]', '[UNK]', 'co', '##rn', '##e', 'u', '##ni', '##que', '.', '[UNK]', 'or', '##ig', '##ine', ',', '[UNK]', ',', '[UNK]', 'de', 'multi', '##ple', '##s', 'in', '##fl', '##ue', '##nce', '##s', ',', 'en', 'part', '##ic', '##ul', '##ier', 'de', 'des', '##cr', '##ip', '##tions', 'd', \"'\", 'an', '##ima', '##ux', 'tel', '##s', 'q', '##ue', 'le', '[UNK]', 'et', 'l', \"'\", 'anti', '##lo', '##pe', ',', 'is', '##sue', '##s', 'de', '[UNK]', 'd', \"'\", 'ex', '##pl', '##ora', '##te', '##urs', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK] licorne, parfois [UNK] unicorne, est une [UNK] [UNK] [UNK] corne unique. [UNK] origine, [UNK], [UNK] de multiples influences, en particulier de descriptions d'animaux tels que le [UNK] et l'antilope, issues de [UNK] d'explorateurs. [SEP]\n",
      "\n",
      "gpt:\n",
      "['la</w>', 'licor', 'ne</w>', ',</w>', 'par', 'fo', 'is</w>', 'no', 'm', 'mee</w>', 'unic', 'or', 'ne</w>', ',</w>', 'est</w>', 'une</w>', 'creature</w>', 'legen', 'da', 'ire</w>', 'a</w>', 'cor', 'ne</w>', 'unique</w>', '.</w>', 'son</w>', 'orig', 'ine</w>', ',</w>', 'controver', 'see</w>', ',</w>', 'resul', 'te</w>', 'de</w>', 'multi', 'ples</w>', 'influences</w>', ',</w>', 'en</w>', 'particu', 'lier</w>', 'de</w>', 'descriptions</w>', 'd</w>', \"'</w>\", 'ani', 'mau', 'x</w>', 'tels</w>', 'que</w>', 'le</w>', 'rhin', 'o', 'cer', 'os</w>', 'et</w>', 'l</w>', \"'</w>\", 'anti', 'lo', 'pe</w>', ',</w>', 'issues</w>', 'de</w>', 'rec', 'its</w>', 'd</w>', \"'</w>\", 'explor', 'ate', 'urs</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "la licorne, parfois nommee unicorne, est une creature legendaire a corne unique. son origine, controversee, resulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinoceros et l'antilope, issues de recits d'explorateurs.\n",
      "\n",
      "gpt2:\n",
      "['La', 'Ġlic', 'orne', ',', 'Ġpar', 'fo', 'is', 'Ġn', 'omm', 'Ã©e', 'Ġunic', 'orne', ',', 'Ġest', 'Ġune', 'Ġcr', 'Ã©', 'ature', 'Ġl', 'Ã©', 'g', 'enda', 'ire', 'ĠÃł', 'Ġcor', 'ne', 'Ġunique', '.', 'ĠSon', 'Ġorig', 'ine', ',', 'Ġcontrovers', 'Ã©e', ',', 'Ġr', 'Ã©s', 'ult', 'e', 'Ġde', 'Ġmulti', 'ples', 'Ġinfluences', ',', 'Ġen', 'Ġpartic', 'ul', 'ier', 'Ġde', 'Ġdescriptions', 'Ġd', \"'\", 'anim', 'aux', 'Ġt', 'els', 'Ġque', 'Ġle', 'Ġrh', 'in', 'oc', 'Ã©', 'ros', 'Ġet', 'Ġl', \"'\", 'ant', 'il', 'ope', ',', 'Ġissues', 'Ġde', 'ĠrÃ©', 'c', 'its', 'Ġd', \"'\", 'expl', 'orate', 'urs', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "La licorne, parfois nommée unicorne, est une créature légendaire à corne unique. Son origine, controversée, résulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinocéros et l'antilope, issues de récits d'explorateurs.\n",
      "\n",
      "transformerxl:\n",
      "['La', 'licorne,', 'parfois', 'nommée', 'unicorne,', 'est', 'une', 'créature', 'légendaire', 'à', 'corne', 'unique.', 'Son', 'origine,', 'controversée,', 'résulte', 'de', 'multiples', 'influences,', 'en', 'particulier', 'de', 'descriptions', \"d'animaux\", 'tels', 'que', 'le', 'rhinocéros', 'et', \"l'antilope,\", 'issues', 'de', 'récits', \"d'explorateurs.\"]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "La <unk> <unk> <unk> <unk> est une <unk> <unk> <unk> <unk> <unk> Son <unk> <unk> <unk> de multiples <unk> en particulier de descriptions <unk> <unk> que le <unk> et <unk> issues de <unk> <unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁La', '▁', 'lic', 'or', 'ne', ',', '▁par', 'fo', 'is', '▁no', 'm', 'mee', '▁', 'uni', 'corn', 'e', ',', '▁', 'est', '▁', 'une', '▁creature', '▁legend', 'aire', '▁a', '▁corn', 'e', '▁unique', '.', '▁Son', '▁origin', 'e', ',', '▁con', 'tro', 'verse', 'e', ',', '▁result', 'e', '▁de', '▁multiple', 's', '▁influences', ',', '▁', 'en', '▁part', 'ic', 'uli', 'er', '▁de', '▁descriptions', '▁', 'd', \"'\", 'ani', 'm', 'aux', '▁', 'tel', 's', '▁', 'que', '▁', 'le', '▁rhino', 'ce', 'ros', '▁', 'et', '▁', 'l', \"'\", 'anti', 'lope', ',', '▁issues', '▁de', '▁', 're', 'cit', 's', '▁', 'd', \"'\", 'ex', 'plo', 'rate', 'ur', 's', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "La licorne, parfois nommee unicorne, est une creature legendaire a corne unique. Son origine, controversee, resulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinoceros et l'antilope, issues de recits d'explorateurs.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['la</w>', 'li', 'cor', 'ne</w>', ',</w>', 'par', 'fo', 'is</w>', 'nom', 'mee</w>', 'unic', 'or', 'ne</w>', ',</w>', 'est</w>', 'une</w>', 'creature</w>', 'legen', 'd', 'aire</w>', 'a</w>', 'cor', 'ne</w>', 'unique</w>', '.</w>', 'son</w>', 'ori', 'g', 'ine</w>', ',</w>', 'contro', 'ver', 'see</w>', ',</w>', 'resul', 'te</w>', 'de</w>', 'multi', 'ples</w>', 'influences</w>', ',</w>', 'en</w>', 'partic', 'u', 'lier</w>', 'de</w>', 'descriptions</w>', 'd</w>', \"'\", 'ani', 'mau', 'x</w>', 't', 'els</w>', 'que</w>', 'le</w>', 'rhin', 'oc', 'eros</w>', 'et</w>', 'l</w>', \"'\", 'anti', 'lo', 'pe</w>', ',</w>', 'issues</w>', 'de</w>', 'rec', 'its</w>', 'd</w>', \"'\", 'explor', 'ate', 'urs</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>la licorne, parfois nommee unicorne, est une creature legendaire a corne unique. son origine, controversee, resulte de multiples influences, en particulier de descriptions d 'animaux tels que le rhinoceros et l 'antilope, issues de recits d 'explorateurs. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['la</w>', 'lic', 'orne</w>', ',</w>', 'parfois</w>', 'nommee</w>', 'unic', 'orne</w>', ',</w>', 'est</w>', 'une</w>', 'creature</w>', 'legen', 'daire</w>', 'a</w>', 'cor', 'ne</w>', 'unique</w>', '.</w>', 'son</w>', 'origine</w>', ',</w>', 'contro', 'ver', 'see</w>', ',</w>', 'resul', 'te</w>', 'de</w>', 'multiples</w>', 'influ', 'ences</w>', ',</w>', 'en</w>', 'particulier</w>', 'de</w>', 'des', 'criptions</w>', 'd</w>', \"'\", 'animaux</w>', 'tels</w>', 'que</w>', 'le</w>', 'rhin', 'oc', 'eros</w>', 'et</w>', 'l</w>', \"'\", 'antil', 'ope</w>', ',</w>', 'issues</w>', 'de</w>', 'rec', 'its</w>', 'd</w>', \"'\", 'explor', 'ateurs</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>la licorne, parfois nommee unicorne, est une creature legendaire a corne unique. son origine, controversee, resulte de multiples influences, en particulier de descriptions d 'animaux tels que le rhinoceros et l 'antilope, issues de recits d 'explorateurs. </s>\n",
      "\n",
      "roberta-base:\n",
      "['La', 'Ġlic', 'orne', ',', 'Ġpar', 'fo', 'is', 'Ġn', 'omm', 'Ã©e', 'Ġunic', 'orne', ',', 'Ġest', 'Ġune', 'Ġcr', 'Ã©', 'ature', 'Ġl', 'Ã©', 'g', 'enda', 'ire', 'ĠÃł', 'Ġcor', 'ne', 'Ġunique', '.', 'ĠSon', 'Ġorig', 'ine', ',', 'Ġcontrovers', 'Ã©e', ',', 'Ġr', 'Ã©s', 'ult', 'e', 'Ġde', 'Ġmulti', 'ples', 'Ġinfluences', ',', 'Ġen', 'Ġpartic', 'ul', 'ier', 'Ġde', 'Ġdescriptions', 'Ġd', \"'\", 'anim', 'aux', 'Ġt', 'els', 'Ġque', 'Ġle', 'Ġrh', 'in', 'oc', 'Ã©', 'ros', 'Ġet', 'Ġl', \"'\", 'ant', 'il', 'ope', ',', 'Ġissues', 'Ġde', 'ĠrÃ©', 'c', 'its', 'Ġd', \"'\", 'expl', 'orate', 'urs', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>La licorne, parfois nommée unicorne, est une créature légendaire à corne unique. Son origine, controversée, résulte de multiples influences, en particulier de descriptions d'animaux tels que le rhinocéros et l'antilope, issues de récits d'explorateurs.</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 4**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "Das Einhorn (lat. unicornis, griech. monókeros) ist ein Fabelwesen von Pferde- oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte.\n",
      "\n",
      "bert-base-cased:\n",
      "['Das', 'Ein', '##horn', '(', 'la', '##t', '.', 'un', '##icorn', '##is', ',', 'g', '##rie', '##ch', '.', 'mon', '##ó', '##ker', '##os', ')', 'is', '##t', 'e', '##in', 'F', '##abe', '##l', '##wes', '##en', 'von', 'P', '##fer', '##de', '-', 'o', '##der', 'Z', '##ie', '##gen', '##ges', '##tal', '##t', 'mit', 'e', '##ine', '##m', 'g', '##era', '##den', 'Horn', 'au', '##f', 'der', 'St', '##irn', '##mit', '##te', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] Das Einhorn ( lat. unicornis, griech. monókeros ) ist ein Fabelwesen von Pferde - oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['Das', 'Ein', '##horn', '(', 'lat', '.', 'unico', '##rnis', ',', 'gr', '##iec', '##h', '.', 'mon', '##ók', '##eros', ')', 'ist', 'ein', 'Fa', '##bel', '##wesen', 'von', 'P', '##ferd', '##e', '-', 'oder', 'Zie', '##gen', '##gest', '##alt', 'mit', 'einem', 'gerade', '##n', 'Horn', 'auf', 'der', 'St', '##irn', '##mitt', '##e', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] Das Einhorn ( lat. unicornis, griech. monókeros ) ist ein Fabelwesen von Pferde - oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', '[UNK]', '(', 'la', '##t', '.', 'u', '##nic', '##or', '##nis', ',', 'g', '##rie', '##ch', '.', '[UNK]', ')', 'is', '##t', 'e', '##in', '[UNK]', 'von', '[UNK]', '-', 'o', '##der', '[UNK]', 'mit', 'e', '##ine', '##m', 'ge', '##rade', '##n', '[UNK]', 'au', '##f', 'der', '[UNK]', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK] [UNK] ( lat. unicornis, griech. [UNK] ) ist ein [UNK] von [UNK] - oder [UNK] mit einem geraden [UNK] auf der [UNK]. [SEP]\n",
      "\n",
      "gpt:\n",
      "['das</w>', 'ein', 'horn</w>', '(</w>', 'l', 'at</w>', '.</w>', 'unic', 'or', 'nis</w>', ',</w>', 'gri', 'e', 'ch</w>', '.</w>', 'mono', 'ker', 'os</w>', ')</w>', 'ist</w>', 'e', 'in</w>', 'fab', 'el', 'wes', 'en</w>', 'von</w>', 'p', 'fer', 'de</w>', '-</w>', 'o', 'der</w>', 'z', 'ie', 'gen', 'gest', 'alt</w>', 'mit</w>', 'e', 'ine', 'm</w>', 'ger', 'aden</w>', 'horn</w>', 'au', 'f</w>', 'der</w>', 'stir', 'n', 'mit', 'te</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "das einhorn ( lat. unicornis, griech. monokeros ) ist ein fabelwesen von pferde - oder ziegengestalt mit einem geraden horn auf der stirnmitte.\n",
      "\n",
      "gpt2:\n",
      "['D', 'as', 'ĠE', 'in', 'horn', 'Ġ(', 'lat', '.', 'Ġunicorn', 'is', ',', 'Ġgri', 'ech', '.', 'Ġmon', 'Ã³', 'ker', 'os', ')', 'Ġis', 't', 'Ġe', 'in', 'ĠF', 'abel', 'w', 'es', 'en', 'Ġvon', 'ĠP', 'fer', 'de', '-', 'Ġo', 'der', 'ĠZ', 'ieg', 'eng', 'est', 'alt', 'Ġmit', 'Ġe', 'inem', 'Ġger', 'aden', 'ĠHorn', 'Ġa', 'uf', 'Ġder', 'ĠStir', 'nm', 'itte', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "Das Einhorn (lat. unicornis, griech. monókeros) ist ein Fabelwesen von Pferde- oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte.\n",
      "\n",
      "transformerxl:\n",
      "['Das', 'Einhorn', '(lat.', 'unicornis,', 'griech.', 'monókeros)', 'ist', 'ein', 'Fabelwesen', 'von', 'Pferde-', 'oder', 'Ziegengestalt', 'mit', 'einem', 'geraden', 'Horn', 'auf', 'der', 'Stirnmitte.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "Das Einhorn <unk> <unk> <unk> <unk> ist ein <unk> von <unk> oder <unk> mit einem <unk> Horn auf der <unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁Da', 's', '▁E', 'in', 'horn', '▁', '(', 'lat', '.', '▁', 'uni', 'corn', 'is', ',', '▁', 'gri', 'ech', '.', '▁mono', 'ker', 'os', ')', '▁is', 't', '▁', 'ein', '▁Fa', 'bel', 'we', 'sen', '▁von', '▁P', 'fer', 'de', '-', '▁', 'o', 'der', '▁Zi', 'ege', 'nge', 'stal', 't', '▁', 'mit', '▁', 'ein', 'em', '▁', 'ger', 'a', 'den', '▁Horn', '▁a', 'uf', '▁', 'der', '▁Stir', 'n', 'mit', 'te', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "Das Einhorn (lat. unicornis, griech. monokeros) ist ein Fabelwesen von Pferde- oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['das</w>', 'ein', 'horn</w>', '(</w>', 'lat', '.</w>', 'unic', 'or', 'nis</w>', ',</w>', 'gri', 'ec', 'h.</w>', 'mon', 'ok', 'eros</w>', ')</w>', 'ist</w>', 'ein</w>', 'fab', 'el', 'wes', 'en</w>', 'von</w>', 'p', 'fer', 'de', '-</w>', 'o', 'der</w>', 'zi', 'e', 'gen', 'ge', 'stal', 't</w>', 'mit</w>', 'e', 'ine', 'm</w>', 'ger', 'aden</w>', 'horn</w>', 'au', 'f</w>', 'der</w>', 'stir', 'n', 'mit', 'te</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>das einhorn ( lat. unicornis, griech. monokeros ) ist ein fabelwesen von pferde- oder ziegengestalt mit einem geraden horn auf der stirnmitte. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['das</w>', 'ein', 'horn</w>', '(</w>', 'lat', '.</w>', 'unic', 'ornis</w>', ',</w>', 'griech', '.</w>', 'mon', 'ok', 'eros</w>', ')</w>', 'ist</w>', 'ein</w>', 'fab', 'el', 'wesen</w>', 'von</w>', 'pfer', 'de', '-</w>', 'oder</w>', 'zie', 'gen', 'gestalt</w>', 'mit</w>', 'einem</w>', 'ger', 'aden</w>', 'horn</w>', 'auf</w>', 'der</w>', 'stir', 'n', 'mitte</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>das einhorn ( lat. unicornis, griech. monokeros ) ist ein fabelwesen von pferde- oder ziegengestalt mit einem geraden horn auf der stirnmitte. </s>\n",
      "\n",
      "roberta-base:\n",
      "['D', 'as', 'ĠE', 'in', 'horn', 'Ġ(', 'lat', '.', 'Ġunicorn', 'is', ',', 'Ġgri', 'ech', '.', 'Ġmon', 'Ã³', 'ker', 'os', ')', 'Ġis', 't', 'Ġe', 'in', 'ĠF', 'abel', 'w', 'es', 'en', 'Ġvon', 'ĠP', 'fer', 'de', '-', 'Ġo', 'der', 'ĠZ', 'ieg', 'eng', 'est', 'alt', 'Ġmit', 'Ġe', 'inem', 'Ġger', 'aden', 'ĠHorn', 'Ġa', 'uf', 'Ġder', 'ĠStir', 'nm', 'itte', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>Das Einhorn (lat. unicornis, griech. monókeros) ist ein Fabelwesen von Pferde- oder Ziegengestalt mit einem geraden Horn auf der Stirnmitte.</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 5**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "\n",
      "bert-base-cased:\n",
      "['Mile', '##y', 'Cyrus', 'was', 'caught', 'shop', '##lifting', 'from', 'Abe', '##rc', '##rom', '##bie', 'and', 'Fi', '##tch', 'on', 'Hollywood', 'Boulevard', 'today', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['Miley', 'Cyrus', 'was', 'caught', 'shop', '##lift', '##ing', 'from', 'Aber', '##cro', '##mbi', '##e', 'and', 'Fi', '##tch', 'on', 'Hollywood', 'Boulevard', 'today', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', '[UNK]', 'was', 'ca', '##ugh', '##t', 'shop', '##li', '##ft', '##ing', 'from', '[UNK]', 'and', '[UNK]', 'on', '[UNK]', '[UNK]', 'today', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK] [UNK] was caught shoplifting from [UNK] and [UNK] on [UNK] [UNK] today. [SEP]\n",
      "\n",
      "gpt:\n",
      "['mi', 'ley</w>', 'cyrus</w>', 'was</w>', 'caught</w>', 'shop', 'lifting</w>', 'from</w>', 'aber', 'crombie</w>', 'and</w>', 'fitch</w>', 'on</w>', 'hollywood</w>', 'boulevard</w>', 'today</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "miley cyrus was caught shoplifting from abercrombie and fitch on hollywood boulevard today.\n",
      "\n",
      "gpt2:\n",
      "['M', 'iley', 'ĠCyrus', 'Ġwas', 'Ġcaught', 'Ġshop', 'lifting', 'Ġfrom', 'ĠAber', 'c', 'rom', 'bie', 'Ġand', 'ĠF', 'itch', 'Ġon', 'ĠHollywood', 'ĠBoulevard', 'Ġtoday', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "\n",
      "transformerxl:\n",
      "['Miley', 'Cyrus', 'was', 'caught', 'shoplifting', 'from', 'Abercrombie', 'and', 'Fitch', 'on', 'Hollywood', 'Boulevard', 'today.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard <unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁Mile', 'y', '▁Cyrus', '▁was', '▁caught', '▁shop', 'lift', 'ing', '▁from', '▁A', 'ber', 'cro', 'm', 'bie', '▁and', '▁Fitch', '▁on', '▁Hollywood', '▁Boulevard', '▁today', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['mi', 'ley</w>', 'cyrus</w>', 'was</w>', 'caught</w>', 'shop', 'lifting</w>', 'from</w>', 'aber', 'crom', 'bie</w>', 'and</w>', 'fit', 'ch</w>', 'on</w>', 'hollywood</w>', 'boulevard</w>', 'today</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>miley cyrus was caught shoplifting from abercrombie and fitch on hollywood boulevard today. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['mil', 'ey</w>', 'cy', 'rus</w>', 'was</w>', 'caught</w>', 'sho', 'p', 'lifting</w>', 'from</w>', 'aber', 'crom', 'bie</w>', 'and</w>', 'fit', 'ch</w>', 'on</w>', 'hollywood</w>', 'boulevard</w>', 'today</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>miley cyrus was caught shoplifting from abercrombie and fitch on hollywood boulevard today. </s>\n",
      "\n",
      "roberta-base:\n",
      "['M', 'iley', 'ĠCyrus', 'Ġwas', 'Ġcaught', 'Ġshop', 'lifting', 'Ġfrom', 'ĠAber', 'c', 'rom', 'bie', 'Ġand', 'ĠF', 'itch', 'Ġon', 'ĠHollywood', 'ĠBoulevard', 'Ġtoday', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 6**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "\n",
      "bert-base-cased:\n",
      "['We', '’', 've', 'trained', 'a', 'large', 'language', 'model', 'called', 'GP', '##T', '-', '2', 'that', 'generates', 'realistic', 'paragraph', '##s', 'of', 'text', ',', 'while', 'also', 'exhibiting', 'zero', 'shot', 'general', '##ization', 'on', 'tasks', 'like', 'machine', 'translation', ',', 'question', 'answering', ',', 'reading', 'com', '##p', '##rehension', ',', 'and', 'sum', '##mar', '##ization', '-', 'problems', 'usually', 'approached', 'by', 'using', 'training', 'data', '##sets', 'and', 'models', 'designed', 'explicitly', 'for', 'these', 'tasks', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] We ’ ve trained a large language model called GPT - 2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['We', '[UNK]', 've', 'trained', 'a', 'large', 'language', 'model', 'called', 'GP', '##T', '-', '2', 'that', 'generate', '##s', 'real', '##istic', 'para', '##graphs', 'of', 'text', ',', 'while', 'also', 'exhibit', '##ing', 'zero', 'shot', 'generali', '##zation', 'on', 'tasks', 'like', 'machine', 'translation', ',', 'question', 'answer', '##ing', ',', 'reading', 'com', '##pre', '##hen', '##sion', ',', 'and', 'sum', '##mari', '##zation', '-', 'problems', 'usually', 'approached', 'by', 'using', 'training', 'data', '##set', '##s', 'and', 'models', 'designed', 'ex', '##plicitly', 'for', 'these', 'tasks', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] We [UNK] ve trained a large language model called GPT - 2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', '[UNK]', 've', 't', '##rain', '##ed', 'a', 'la', '##rge', 'language', 'model', 'call', '##ed', '[UNK]', '-', '2', 'that', 'ge', '##ner', '##ate', '##s', 'real', '##ist', '##ic', 'pa', '##ra', '##g', '##raph', '##s', 'of', 'text', ',', 'w', '##hi', '##le', 'also', 'ex', '##hi', '##bit', '##ing', 'zero', 'sh', '##ot', 'general', '##i', '##za', '##tion', 'on', 'ta', '##sk', '##s', 'like', 'machine', 't', '##ran', '##sl', '##ation', ',', 'q', '##ue', '##st', '##ion', 'an', '##s', '##wer', '##ing', ',', 'reading', 'com', '##pr', '##eh', '##ens', '##ion', ',', 'and', 'su', '##mma', '##ri', '##za', '##tion', '-', 'pro', '##ble', '##ms', 'us', '##ual', '##ly', 'app', '##ro', '##ach', '##ed', 'by', 'us', '##ing', 't', '##rain', '##ing', 'data', '##set', '##s', 'and', 'model', '##s', 'design', '##ed', 'ex', '##pl', '##ic', '##it', '##ly', 'for', 'the', '##se', 'ta', '##sk', '##s', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK] [UNK] ve trained a large language model called [UNK] - 2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. [SEP]\n",
      "\n",
      "gpt:\n",
      "['we</w>', '’</w>', 've</w>', 'trained</w>', 'a</w>', 'large</w>', 'language</w>', 'model</w>', 'called</w>', 'g', 'pt</w>', '-</w>', '2</w>', 'that</w>', 'gener', 'ates</w>', 'realistic</w>', 'paragraphs</w>', 'of</w>', 'text</w>', ',</w>', 'while</w>', 'also</w>', 'exhibiting</w>', 'zero</w>', 'shot</w>', 'gener', 'alization</w>', 'on</w>', 'tasks</w>', 'like</w>', 'machine</w>', 'translation</w>', ',</w>', 'question</w>', 'answering</w>', ',</w>', 'reading</w>', 'comprehension</w>', ',</w>', 'and</w>', 'summari', 'zation</w>', '-</w>', 'problems</w>', 'usually</w>', 'approached</w>', 'by</w>', 'using</w>', 'training</w>', 'dat', 'a', 'sets</w>', 'and</w>', 'models</w>', 'designed</w>', 'explicitly</w>', 'for</w>', 'these</w>', 'tasks</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "we <unk>ve trained a large language model called gpt - 2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "\n",
      "gpt2:\n",
      "['We', 'âĢ', 'Ļ', 've', 'Ġtrained', 'Ġa', 'Ġlarge', 'Ġlanguage', 'Ġmodel', 'Ġcalled', 'ĠG', 'PT', '-', '2', 'Ġthat', 'Ġgenerates', 'Ġrealistic', 'Ġparagraphs', 'Ġof', 'Ġtext', ',', 'Ġwhile', 'Ġalso', 'Ġexhibiting', 'Ġzero', 'Ġshot', 'Ġgeneral', 'ization', 'Ġon', 'Ġtasks', 'Ġlike', 'Ġmachine', 'Ġtranslation', ',', 'Ġquestion', 'Ġanswering', ',', 'Ġreading', 'Ġcomprehension', ',', 'Ġand', 'Ġsummar', 'ization', 'Ġ-', 'Ġproblems', 'Ġusually', 'Ġapproached', 'Ġby', 'Ġusing', 'Ġtraining', 'Ġdatasets', 'Ġand', 'Ġmodels', 'Ġdesigned', 'Ġexplicitly', 'Ġfor', 'Ġthese', 'Ġtasks', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "\n",
      "transformerxl:\n",
      "['We’ve', 'trained', 'a', 'large', 'language', 'model', 'called', 'GPT-2', 'that', 'generates', 'realistic', 'paragraphs', 'of', 'text,', 'while', 'also', 'exhibiting', 'zero', 'shot', 'generalization', 'on', 'tasks', 'like', 'machine', 'translation,', 'question', 'answering,', 'reading', 'comprehension,', 'and', 'summarization', '-', 'problems', 'usually', 'approached', 'by', 'using', 'training', 'datasets', 'and', 'models', 'designed', 'explicitly', 'for', 'these', 'tasks.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "<unk> trained a large language model called <unk> that generates realistic paragraphs of <unk> while also exhibiting zero shot generalization on tasks like machine <unk> question <unk> reading <unk> and <unk> - problems usually approached by using training datasets and models designed explicitly for these <unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁We', '’', 've', '▁trained', '▁a', '▁large', '▁language', '▁model', '▁called', '▁G', 'PT', '-', '2', '▁that', '▁generates', '▁realistic', '▁paragraph', 's', '▁of', '▁text', ',', '▁while', '▁also', '▁exhibit', 'ing', '▁zero', '▁shot', '▁general', 'ization', '▁on', '▁tasks', '▁like', '▁machine', '▁translation', ',', '▁question', '▁answering', ',', '▁reading', '▁comprehension', ',', '▁and', '▁sum', 'mar', 'ization', '▁', '-', '▁problems', '▁usually', '▁approached', '▁by', '▁using', '▁training', '▁dataset', 's', '▁and', '▁models', '▁designed', '▁explicitly', '▁for', '▁these', '▁tasks', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['we</w>', \"'ve</w>\", 'trained</w>', 'a</w>', 'large</w>', 'language</w>', 'model</w>', 'called</w>', 'gp', 't-', '2</w>', 'that</w>', 'generates</w>', 'realistic</w>', 'paragra', 'phs</w>', 'of</w>', 'text</w>', ',</w>', 'while</w>', 'also</w>', 'exhibiting</w>', 'zero</w>', 'shot</w>', 'gener', 'alization</w>', 'on</w>', 'tasks</w>', 'like</w>', 'machine</w>', 'translation</w>', ',</w>', 'question</w>', 'answering</w>', ',</w>', 'reading</w>', 'comprehen', 'sion</w>', ',</w>', 'and</w>', 'summari', 'zation</w>', '-</w>', 'problems</w>', 'usually</w>', 'approached</w>', 'by</w>', 'using</w>', 'training</w>', 'dat', 'as', 'ets</w>', 'and</w>', 'models</w>', 'designed</w>', 'explicitly</w>', 'for</w>', 'these</w>', 'tasks</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>we've trained a large language model called gpt-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['we</w>', \"'ve</w>\", 'train', 'ed</w>', 'a</w>', 'large</w>', 'language</w>', 'model</w>', 'called</w>', 'gp', 't-', '2</w>', 'that</w>', 'gener', 'ates</w>', 're', 'alist', 'ic</w>', 'parag', 'rap', 'hs</w>', 'of</w>', 'text</w>', ',</w>', 'while</w>', 'also</w>', 'ex', 'hibit', 'ing</w>', 'zero</w>', 'shot</w>', 'gener', 'aliz', 'ation</w>', 'on</w>', 't', 'asks</w>', 'like</w>', 'machine</w>', 'trans', 'lation</w>', ',</w>', 'question</w>', 'ans', 'wering</w>', ',</w>', 'reading</w>', 'comprehen', 'sion</w>', ',</w>', 'and</w>', 'sum', 'mar', 'iz', 'ation</w>', '-</w>', 'problems</w>', 'usually</w>', 'approached</w>', 'by</w>', 'using</w>', 'training</w>', 'dat', 'as', 'ets</w>', 'and</w>', 'model', 's</w>', 'desig', 'ned</w>', 'explic', 'it', 'ly</w>', 'for</w>', 'these</w>', 't', 'asks</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>we've trained a large language model called gpt-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually <unk>by using training datasets and models designed explicitly for these tasks. </s>\n",
      "\n",
      "roberta-base:\n",
      "['We', 'âĢ', 'Ļ', 've', 'Ġtrained', 'Ġa', 'Ġlarge', 'Ġlanguage', 'Ġmodel', 'Ġcalled', 'ĠG', 'PT', '-', '2', 'Ġthat', 'Ġgenerates', 'Ġrealistic', 'Ġparagraphs', 'Ġof', 'Ġtext', ',', 'Ġwhile', 'Ġalso', 'Ġexhibiting', 'Ġzero', 'Ġshot', 'Ġgeneral', 'ization', 'Ġon', 'Ġtasks', 'Ġlike', 'Ġmachine', 'Ġtranslation', ',', 'Ġquestion', 'Ġanswering', ',', 'Ġreading', 'Ġcomprehension', ',', 'Ġand', 'Ġsummar', 'ization', 'Ġ-', 'Ġproblems', 'Ġusually', 'Ġapproached', 'Ġby', 'Ġusing', 'Ġtraining', 'Ġdatasets', 'Ġand', 'Ġmodels', 'Ġdesigned', 'Ġexplicitly', 'Ġfor', 'Ġthese', 'Ġtasks', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 7**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "こんにちは世界。\n",
      "\n",
      "bert-base-cased:\n",
      "['こ', '##ん', '##に', '##ち', '##は', '[UNK]', '[UNK]', '。']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] こんにちは [UNK] [UNK] 。 [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['こ', '##ん', '##に', '##ち', '##は', '世', '界', '。']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] こんにちは 世 界 。 [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['こ', '##ん', '##に', '##ちは', '世', '界', '。']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] こんにちは 世 界 。 [SEP]\n",
      "\n",
      "gpt:\n",
      "['こ', 'ん', 'に', 'ち', 'は</w>', '世</w>', '界</w>', '。</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "\n",
      "gpt2:\n",
      "['ãģĵ', 'ãĤĵ', 'ãģ«', 'ãģ', '¡', 'ãģ¯', 'ä¸', 'ĸ', 'çķ', 'Į', 'ãĢĤ']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "こんにちは世界。\n",
      "\n",
      "transformerxl:\n",
      "['こんにちは世界。']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "<unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁', 'こんにちは世界。']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "<unk><sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['こ', 'ん', 'に', 'ち', 'は</w>', '世</w>', '界</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s><unk><unk><unk><unk><unk><unk><unk>. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['こ', 'ん', 'に', 'ち', 'は</w>', '世</w>', '界</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>こんにちは 世 界. </s>\n",
      "\n",
      "roberta-base:\n",
      "['ãģĵ', 'ãĤĵ', 'ãģ«', 'ãģ', '¡', 'ãģ¯', 'ä¸', 'ĸ', 'çķ', 'Į', 'ãĢĤ']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>こんにちは世界。</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 8**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "你好，世界。\n",
      "\n",
      "bert-base-cased:\n",
      "['[UNK]', '[UNK]', '，', '[UNK]', '[UNK]', '。']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] [UNK] [UNK] ， [UNK] [UNK] 。 [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['你', '好', '，', '世', '界', '。']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] 你 好 ， 世 界 。 [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['你', '好', '，', '世', '界', '。']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] 你 好 ， 世 界 。 [SEP]\n",
      "\n",
      "gpt:\n",
      "['你</w>', '好</w>', '，</w>', '世</w>', '界</w>', '。</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "<unk><unk><unk><unk><unk><unk>\n",
      "\n",
      "gpt2:\n",
      "['ä½', 'ł', 'å¥', '½', 'ï', '¼', 'Į', 'ä¸', 'ĸ', 'çķ', 'Į', 'ãĢĤ']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "你好，世界。\n",
      "\n",
      "transformerxl:\n",
      "['你好，世界。']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "<unk>\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁', '你好', ',', '世界。']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "<unk>,<unk><sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['你</w>', '好</w>', ',</w>', '世</w>', '界</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s><unk><unk>, <unk><unk>. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['你</w>', '好</w>', ',</w>', '世</w>', '界</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>你 好, 世 界. </s>\n",
      "\n",
      "roberta-base:\n",
      "['ä½', 'ł', 'å¥', '½', 'ï', '¼', 'Į', 'ä¸', 'ĸ', 'çķ', 'Į', 'ãĢĤ']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>你好，世界。</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence 9**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "Hello, world.\n",
      "\n",
      "bert-base-cased:\n",
      "['Hello', ',', 'world', '.']\n",
      "\n",
      "bert-base-cased (recover):\n",
      "[CLS] Hello, world. [SEP]\n",
      "\n",
      "bert-base-multilingual-cased:\n",
      "['Hello', ',', 'world', '.']\n",
      "\n",
      "bert-base-multilingual-cased (recover):\n",
      "[CLS] Hello, world. [SEP]\n",
      "\n",
      "bert-base-chinese:\n",
      "['[UNK]', ',', 'world', '.']\n",
      "\n",
      "bert-base-chinese (recover):\n",
      "[CLS] [UNK], world. [SEP]\n",
      "\n",
      "gpt:\n",
      "['hello</w>', ',</w>', 'world</w>', '.</w>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt (recover):\n",
      "hello, world.\n",
      "\n",
      "gpt2:\n",
      "['Hello', ',', 'Ġworld', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 (recover):\n",
      "Hello, world.\n",
      "\n",
      "transformerxl:\n",
      "['Hello,', 'world.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformerxl (recover):\n",
      "<unk> world.\n",
      "\n",
      "xlnet-base-cased:\n",
      "['▁', 'Hello', ',', '▁world', '.']\n",
      "\n",
      "xlnet-base-cased (recover):\n",
      "Hello, world.<sep><cls>\n",
      "\n",
      "xlm-mlm-en:\n",
      "['hello</w>', ',</w>', 'world</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-en (recover):\n",
      "</s>hello, world. </s>\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024:\n",
      "['hello</w>', ',</w>', 'world</w>', '.</w>']\n",
      "\n",
      "xlm-mlm-tlm-xnli15-1024 (recover):\n",
      "</s>hello, world. </s>\n",
      "\n",
      "roberta-base:\n",
      "['Hello', ',', 'Ġworld', '.']\n",
      "\n",
      "roberta-base (recover):\n",
      "<s>Hello, world.</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(texts):\n",
    "    show_bold(f'Sentence {i}')\n",
    "    transformers_tokenize_sentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
