{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "window_size=5\n",
    "file_path = \"../data/产出导向法.txt\"\n",
    "mi_path = \"../results/互信息.txt\"\n",
    "\n",
    "# 对语料进行处理\n",
    "def build_corpus():\n",
    "    # 分词\n",
    "    def cut_words(sent):\n",
    "        return [word.word for word in pseg.cut(sent) if word.flag[0] not in ['x','w','p','u','c']]\n",
    "    sents = [cut_words(sent) for sent in open(file_path,encoding='utf-8').read().split('\\n')]\n",
    "#     [['a','the'], ['this','that']]\n",
    "    return sents\n",
    "# 统计相关的词频\n",
    "def count_words(sents):\n",
    "    words_all = list()  # words_all = []\n",
    "    for sent in sents:\n",
    "        words_all.extend(sent)  # ['a','the', 'this','that']\n",
    "    word_dict = {item[0]:item[1] for item in collections.Counter(words_all).most_common()} # {'a':1,'the':1}\n",
    "    return words_all,word_dict\n",
    "# 读取语料\n",
    "def build_cowords(sents):\n",
    "    train_data = list()\n",
    "    for sent in sents:\n",
    "        for index,word in enumerate(sent):  # [(0,'a'), (1,'the')]\n",
    "            if index <window_size:\n",
    "                left = sent[:index]    # John likes to watch movies Mary likes too.John also likes to watch football games.\n",
    "            else:\n",
    "                left = sent[index-window_size:index]\n",
    "            if index+window_size>len(sent):\n",
    "                right = sent[index+1:]\n",
    "            else:\n",
    "                right = sent[index+1:index+1+window_size+1]\n",
    "            data = left+right+[sent[index]] # left = [to watch movies Mary likes], right = [John also likes to watch], [too]\n",
    "            # data = [to watch movies Mary likes John also likes to watch too]\n",
    "            if '' in data:\n",
    "                data.remove('')\n",
    "            train_data.append(data)\n",
    "    return train_data\n",
    "# 统计共现矩阵\n",
    "def count_cowords(train_data):\n",
    "    co_dict = dict()\n",
    "    for index,data in enumerate(train_data):\n",
    "        for index_pre in range(len(data)):\n",
    "            for index_post in range(len(data)):\n",
    "                if data[index_pre] not in co_dict:\n",
    "                    co_dict[data[index_pre]] = data[index_post]\n",
    "                else:\n",
    "                    co_dict[data[index_pre]] +=\"@\" +data[index_post]\n",
    "    return co_dict\n",
    "\n",
    "# co_dict = {'to': 'to@watch@movies@.....too', 'watch':'to@watch@movies@.....too', 'movies':'to@watch@movies@.....too'}\n",
    "\n",
    "# 计算互信息\n",
    "def compute_words_mi(word_dict,co_dict,sum_tf):\n",
    "    def build_dict(words):\n",
    "        return {item[0]:item[1] for item in collections.Counter(words).most_common()}\n",
    "    mi_dict = dict()\n",
    "    for word,co_words in co_dict.items():\n",
    "        co_word_dict = build_dict(co_words.split('@'))  # [to watch movies Mary likes John also likes to watch too]\n",
    "        mis_dict = {}\n",
    "        p1 = word_dict[word]/sum_tf  # 单词的出现概率，也就是P(x)\n",
    "        for co_word,co_tf in co_word_dict.items():\n",
    "            if co_word == word:\n",
    "                continue\n",
    "            p2 = word_dict[co_word]/sum_tf  # 与单词x在一个词窗里面共现的单词的出现概率，也就是P(y)\n",
    "            p12 = co_tf/sum_tf  # 联合概率P(x,y)\n",
    "            mi = math.log2(p12)-math.log2(p1)-math.log2(p2)\n",
    "            mis_dict[co_word] = mi\n",
    "        mis_dict = sorted(mis_dict.items(),key=lambda asd:asd[1],reverse=True)\n",
    "        mi_dict[word] = mis_dict   # {'to':{'watch':5, 'likes':4, 'also':3}, 'like':{'to': 10, 'are':5}}\n",
    "    return mi_dict\n",
    "# 将产生的互信息文件进行保存\n",
    "def save_mi(mi_dict):\n",
    "    f = open(mi_path,'w+')\n",
    "    for word,co_words in mi_dict.items():\n",
    "        con_infos = [item[0]+\"@\"+str(item[1]) for item in co_words]\n",
    "        f.write(word+'\\t'+','.join(con_infos)+'\\n')\n",
    "    f.close()\n",
    "#主函数\n",
    "\n",
    "sents = build_corpus()\n",
    "words_all,word_dict = count_words(sents)\n",
    "sum_tf = len(words_all)\n",
    "train_data = build_cowords(sents)\n",
    "co_dict = count_cowords(train_data)\n",
    "mi_dict = compute_words_mi(word_dict,co_dict,sum_tf)\n",
    "save_mi(mi_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
