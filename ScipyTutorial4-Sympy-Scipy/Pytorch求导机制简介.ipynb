{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy import Matrix,MatrixSymbol\n",
    "from sympy import symbols, diff, init_printing\n",
    "\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian matrix\n",
    "\n",
    "假设存在函数$f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$, 输入变量$\\mathbf{x} \\in \\mathbb{R}^{n}$, 输出变量为$\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^{m}$, 函数$f$的`Jacobian`矩阵为$m \\times n$矩阵, 定义为:\n",
    "\n",
    "$$\\mathbf{J}=\\left[ \\begin{array}{ccc}{\\frac{\\partial \\mathbf{f}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial \\mathbf{f}}{\\partial x_{n}}}\\end{array}\\right]=\\left[ \\begin{array}{ccc}{\\frac{\\partial f_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{1}}{\\partial x_{n}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial f_{m}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{m}}{\\partial x_{n}}}\\end{array}\\right]$$\n",
    "\n",
    "或者有,\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{i j}=\\frac{\\partial f_{i}}{\\partial x_{j}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例1\n",
    "下面看一个例子,\n",
    "\n",
    "假设:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} x & \\rightarrow y \\rightarrow z \\\\ y=& f(x), z=g(y) \\\\ f& : \\mathbb{R}^{a}  \\rightarrow \\mathbb{R}^{b} \\\\ g & : \\mathbb{R}^{b} \\rightarrow \\mathbb{R}^{c} \\end{aligned}\n",
    "$$\n",
    "\n",
    "则有:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{\\mathbf{x} \\rightarrow \\mathbf{y}}=\\left[ \\begin{array}{ccc}{\\frac{\\partial f_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{1}}{\\partial x_{a}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial f_{b}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{b}}{\\partial x_{a}}}\\end{array}\\right], \\mathbf{J}_{\\mathbf{y} \\rightarrow \\mathbf{z}}=\\left[ \\begin{array}{ccc}{\\frac{\\partial g_{1}}{\\partial y_{1}}} & {\\cdots} & {\\frac{\\partial g_{1}}{\\partial y_{b}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial g_{c}}{\\partial y_{1}}} & {\\cdots} & {\\frac{\\partial g_{c}}{\\partial y_{b}}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "令:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}=\\left[ \\begin{array}{ll}{x_{1},} & {x_{2}}\\end{array}\\right]=\\left[ \\begin{array}{ll}{1,} & {2}\\end{array}\\right], \\mathbf{y}=\\left[ \\begin{array}{cc}{2 x_{1}+x_{2}^{2},} & {x_{1}^{2}+2 x_{2}^{3}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "所以:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{\\mathbf{x} \\rightarrow \\mathbf{y}}=\\left[ \\begin{array}{cc}{2,} & {2 x_{2}} \\\\ {2 x_{1},} & {6 x_{2}^{2}}\\end{array}\\right]=\\left[ \\begin{array}{cc}{2,} & {4} \\\\ {2,} & {24}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "`PyTorch`不能直接计算`Jacobian`矩阵, 但是可以计算:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}^{T} \\cdot v=\\left( \\begin{array}{ccc}{\\frac{\\partial y_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial y_{m}}{\\partial x_{1}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial y_{1}}{\\partial x_{n}}} & {\\cdots} & {\\frac{\\partial y_{m}}{\\partial x_{n}}}\\end{array}\\right) \\left( \\begin{array}{c}{\\frac{\\partial l}{\\partial y_{1}}} \\\\ {\\vdots} \\\\ {\\frac{\\partial l}{\\partial y_{m}}}\\end{array}\\right)=\\left( \\begin{array}{c}{\\frac{\\partial l}{\\partial x_{1}}} \\\\ {\\vdots} \\\\ {\\frac{\\partial l}{\\partial x_{n}}}\\end{array}\\right) = v^T \\cdot \\mathbf{J}\n",
    "$$\n",
    "\n",
    "因此下面的代码:\n",
    "\n",
    "```python\n",
    "y.backward(torch.tensor([[k1, k2]]))\n",
    "```\n",
    "\n",
    "结果为:\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{ll}{k_{1},} & {k_{2}}\\end{array}\\right]\n",
    "\\left[ \\begin{array}{cc}{2,} & {2 x_{2}} \\\\ {2 x_{1},} & {6 x_{2}^{2}}\\end{array}\\right]\n",
    "=\n",
    "k_{1} *\\left[2,2 x_{2}\\right]+k 2 *\\left[2 x_{1}, 6 x_{2}^{2}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Sympy推导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(symbols('x:2'))\n",
    "Y = sp.zeros(2, 1)\n",
    "Y[0] = 2*X[0] + X[1]**2\n",
    "Y[1] = X[0]**2 + 2*X[1]**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 x_{0} + x_{1}^{2}\\\\x_{0}^{2} + 2 x_{1}^{3}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡         2 ⎤\n",
       "⎢2⋅x₀ + x₁  ⎥\n",
       "⎢           ⎥\n",
       "⎢  2       3⎥\n",
       "⎣x₀  + 2⋅x₁ ⎦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 & 2 x_{1}\\\\2 x_{0} & 6 x_{1}^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡ 2    2⋅x₁ ⎤\n",
       "⎢           ⎥\n",
       "⎢          2⎥\n",
       "⎣2⋅x₀  6⋅x₁ ⎦"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_Y = Y.jacobian(X)\n",
    "J_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Matrix(symbols('v:2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = V.T * J_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}4 & 28\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "[4  28]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT.subs({X[0]:1, X[1]:2, V[0]:1, V[1]:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}6 & 32\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "[6  32]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT.subs({X[0]:1, X[1]:2, V[0]:2, V[1]:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用torch验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y = torch.zeros_like(x)\n",
    "\n",
    "x1 = x[0, 0]\n",
    "x2 = x[0, 1]\n",
    "y[0, 0] = 2*x1+x2**2\n",
    "y[0, 1] = x1**2+2*x2**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.]], requires_grad=True),\n",
       " tensor([[ 6., 17.]], grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if x.grad:\n",
    "    x.grad.zero_()\n",
    "y.backward(torch.tensor([[1.0, 0.0]]), retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., 24.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "y.backward(torch.tensor([[0.0, 1.0]]), retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6., 52.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "y.backward(torch.tensor([[1.0, 2.0]]), retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6., 32.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "y.backward(torch.tensor([[2.0, 1.0]]), retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4., 28.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "y.backward(torch.tensor([[1.0, 1.0]]), retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例2\n",
    "\n",
    "假设有: \n",
    "\n",
    "$$\n",
    "\\mathbf{c}^{m \\times n}=\\mathbf{a}^{m \\times k} \\times \\mathbf{b}^{k \\times n},\n",
    "$$\n",
    "\n",
    "即:\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{ccc}{c_{1,1}} & {\\cdots} & {c_{1, n}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {c_{m, 1}} & {\\cdots} & {c_{m, n}}\\end{array}\\right]=\\left[ \\begin{array}{ccc}{a_{1,1}} & {\\cdots} & {a_{1, k}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {a_{m, 1}} & {\\cdots} & {a_{m, k}}\\end{array}\\right] \\times \\left[ \\begin{array}{ccc}{b_{1,1}} & {\\cdots} & {b_{1, n}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {b_{k, 1}} & {\\cdots} & {b_{k, n}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "`PyTorch`自动求导过程如下:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{1,1}}{\\partial \\mathbf{b}^{k \\times n}}=\\left[ \\begin{array}{ccc}{\\frac{\\partial c_{1,1}}{\\partial b_{1,1}}} & {\\cdots} & {\\frac{\\partial c_{1,1}}{\\partial b_{1, n}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial c_{1,1}}{\\partial b_{k, 1}}} & {\\cdots} & {\\frac{\\partial c_{1,1}}{\\partial b_{k, n}}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{b}^{k \\times n} \\mathbf{c}}=\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{\\partial c_{i, j}}{\\partial \\mathbf{b}^{k \\times n}}\n",
    "$$\n",
    "\n",
    "下面手动计算一下, 令:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}=\\left[ \\begin{array}{ll}{a_{1,1}} & {a_{1,2}} \\\\ {a_{2,1}} & {a_{2,2}}\\end{array}\\right]=\\left[ \\begin{array}{ll}{1} & {2} \\\\ {3} & {4}\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b}=\\left[ \\begin{array}{lll}{b_{1,1}} & {b_{1,2}} & {b_{1,3}} \\\\ {b_{2,1}} & {b_{2,2}} & {b_{2,3}}\\end{array}\\right]=\\left[ \\begin{array}{lll}{1} & {2} & {3} \\\\ {4} & {5} & {6}\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{c}=\\left[ \\begin{array}{lll}{c_{1,1}} & {c_{1,2}} & {c_{1,3}} \\\\ {c_{2,1}} & {c_{2,2}} & {c_{2,3}}\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "根据乘法公式:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} c_{1,1} &=a_{1,1} b_{1,1}+a_{1,2} b_{2,1} \\\\ c_{1,2} &=a_{1,1} b_{1,2}+a_{1,2} b_{2,2} \\\\ c_{1,3} &=a_{1,1} b_{1,3}+a_{1,2} b_{2,3} \\\\ c_{2,1} &=a_{2,1} b_{1,1}+a_{2,2} b_{2,2} \\\\ c_{2,2} &=a_{2,1} b_{1,2}+a_{2,2} b_{2,2} \\\\ c_{2,3} &=a_{2,1} b_{1,3}+a_{2,2} b_{2,2} \\end{aligned}\n",
    "$$\n",
    "\n",
    "因此导数为:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{1,1}}{\\partial \\mathbf{b}}=\\left[ \\begin{array}{lll}{a_{1,1}} & {0} & {0} \\\\ {a_{1,2}} & {0} & {0}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{1,2}}{\\partial \\mathbf{b}}=\\left[ \\begin{array}{lll}{0} & {a_{1,1}} & {0} \\\\ {0} & {a_{1,2}} & {0}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{1,3}}{\\partial \\mathbf{b}}=\\left[ \\begin{array}{lll}{0} & {0} & {a_{1,1}} \\\\ {0} & {0} & {a_{1,2}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{2,1}}{\\partial \\mathbf{b}}=\\left[ \\begin{array}{lll}{a_{2,1}} & {0} & {0} \\\\ {a_{2,2}} & {0} & {0}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{2,2}}{\\partial \\mathbf{b}}=\\left[ \\begin{array}{lll}{0} & {a_{2,1}} & {0} \\\\ {0} & {a_{2,2}} & {0}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{2,3}}{\\partial \\mathbf{b}}=\\left[ \\begin{array}{lll}{0} & {0} & {a_{2,1}} \\\\ {0} & {0} & {a_{2,2}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "将上面各式加起来可得:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{b}^{2 \\times 3} \\mathbf{c}}=\\sum_{i=1}^{2} \\sum_{j=1}^{3} \\frac{\\partial c_{i, j}}{\\partial \\mathbf{b}^{k \\times n}}\n",
    "=\n",
    "\\left[ \\begin{array}{ccc}{a_{1,1}+a_{2,1}} & {a_{1,1}+a_{2,1}} & {a_{1,1}+a_{2,1}} \\\\ {a_{1,2}+a_{2,2}} & {a_{1,2}+a_{2,2}} & {a_{1,2}+a_{2,2}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "上式就是`b.grad`的结果, 其中`c.backward(torch.ons_like(c))`中`backward()`的参数是与$\\mathbf{c}^{m \\times n}$ 形状相同且全为1的矩阵, 其中矩阵每个位置的值**对应**上面各式的系数. 将`c.backward(v)`的参数记为$\\mathbf{v}^{m \\times n}$:\n",
    "\n",
    "$$\n",
    "\\text{b.grad} =  \n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n} v_{i, j} \\times \\frac{\\partial c_{i, j}}{\\partial \\mathbf{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Sympy推导上面的公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_matrix(m0, m1):\n",
    "    \"\"\"\n",
    "    Return d_m0/d_m1.\n",
    "    \n",
    "    Args:\n",
    "        m0, m1 are sympy.Matrix\n",
    "    \"\"\"\n",
    "    shape = m0.shape\n",
    "    out = Matrix.zeros(*m1.shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            out += diff(m0[i, j], m1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Matrix(MatrixSymbol('a', 2, 2))\n",
    "B = Matrix(MatrixSymbol('b', 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}b_{0, 0} + b_{0, 1} + b_{0, 2} & b_{1, 0} + b_{1, 1} + b_{1, 2}\\\\b_{0, 0} + b_{0, 1} + b_{0, 2} & b_{1, 0} + b_{1, 1} + b_{1, 2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡b₀₀ + b₀₁ + b₀₂  b₁₀ + b₁₁ + b₁₂⎤\n",
       "⎢                                ⎥\n",
       "⎣b₀₀ + b₀₁ + b₀₂  b₁₀ + b₁₁ + b₁₂⎦"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_dA = diff_matrix(C, A)\n",
    "dC_dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}6 & 15\\\\6 & 15\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡6  15⎤\n",
       "⎢     ⎥\n",
       "⎣6  15⎦"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_dA.subs({k:v for k, v in zip(B, range(1, 7))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a_{0, 0} + a_{1, 0} & a_{0, 0} + a_{1, 0} & a_{0, 0} + a_{1, 0}\\\\a_{0, 1} + a_{1, 1} & a_{0, 1} + a_{1, 1} & a_{0, 1} + a_{1, 1}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡a₀₀ + a₁₀  a₀₀ + a₁₀  a₀₀ + a₁₀⎤\n",
       "⎢                               ⎥\n",
       "⎣a₀₁ + a₁₁  a₀₁ + a₁₁  a₀₁ + a₁₁⎦"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_dB = diff_matrix(C, B)\n",
    "dC_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}4 & 4 & 4\\\\6 & 6 & 6\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡4  4  4⎤\n",
       "⎢       ⎥\n",
       "⎣6  6  6⎦"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_dB.subs({k:v for k,v in zip(A,range(1, 5))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在Pytorch中验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], \n",
    "                 dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]], \n",
    "                 dtype=torch.float32, requires_grad=True)\n",
    "c = torch.mm(a, b)\n",
    "v = torch.ones_like(c)\n",
    "c.backward(v, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6., 15.],\n",
       "        [ 6., 15.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_metadata": {
   "author": "Hongliang Yang",
   "title": "PyTorch 求导机制"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
